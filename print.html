<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Kubernetes Cluster API Provider for Oracle Cloud Infrastructure</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="./src/css/mdbook-admonish.css">
        <link rel="stylesheet" href="././mdbook-admonish.css">
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded affix "><a href="prerequisites.html">Prerequisites</a></li><li class="chapter-item expanded "><a href="gs/gs.html"><strong aria-hidden="true">1.</strong> Getting Started</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="gs/overview.html"><strong aria-hidden="true">1.1.</strong> Overview</a></li><li class="chapter-item expanded "><a href="gs/custom-machine-images.html"><strong aria-hidden="true">1.2.</strong> Prepare custom OCI images</a></li><li class="chapter-item expanded "><a href="gs/iam.html"><strong aria-hidden="true">1.3.</strong> Configure users and policies</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="gs/iam/iam-oke.html"><strong aria-hidden="true">1.3.1.</strong> Configure policies for an OKE cluster</a></li><li class="chapter-item expanded "><a href="gs/iam/iam-self-provisioned.html"><strong aria-hidden="true">1.3.2.</strong> Configure policies for a self-provisioned cluster</a></li><li class="chapter-item expanded "><a href="gs/iam/iam-ocid.html"><strong aria-hidden="true">1.3.3.</strong> User configuration and OCIDs</a></li></ol></li><li class="chapter-item expanded "><a href="gs/provision-mgmt-cluster.html"><strong aria-hidden="true">1.4.</strong> Provision a management cluster</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="gs/mgmt/mgmt-kind.html"><strong aria-hidden="true">1.4.1.</strong> Provision a management cluster with kind</a></li><li class="chapter-item expanded "><a href="gs/mgmt/mgmt-oke.html"><strong aria-hidden="true">1.4.2.</strong> Provision a management cluster with OKE</a></li></ol></li><li class="chapter-item expanded "><a href="gs/install-cluster-api.html"><strong aria-hidden="true">1.5.</strong> Install Cluster API for Oracle Cloud Infrastructure</a></li><li class="chapter-item expanded "><a href="gs/create-workload-cluster.html"><strong aria-hidden="true">1.6.</strong> Create Workload Cluster</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="gs/create-mhc-workload-cluster.html"><strong aria-hidden="true">1.6.1.</strong> MachineHealthChecks</a></li><li class="chapter-item expanded "><a href="gs/create-gpu-workload-cluster.html"><strong aria-hidden="true">1.6.2.</strong> Create GPU Workload Cluster</a></li><li class="chapter-item expanded "><a href="gs/create-windows-workload-cluster.html"><strong aria-hidden="true">1.6.3.</strong> Create Windows Workload Cluster</a></li></ol></li><li class="chapter-item expanded "><a href="gs/create-workload-templates.html"><strong aria-hidden="true">1.7.</strong> Create Workload Templates</a></li><li class="chapter-item expanded "><a href="gs/externally-managed-cluster-infrastructure.html"><strong aria-hidden="true">1.8.</strong> Using externally managed infrastructure</a></li><li class="chapter-item expanded "><a href="gs/install-oci-ccm.html"><strong aria-hidden="true">1.9.</strong> Install Oracle Cloud Infrastructure Cloud Controller Manager</a></li><li class="chapter-item expanded "><a href="gs/install-csi.html"><strong aria-hidden="true">1.10.</strong> Install Container Storage Interface (CSI)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="gs/pvc-bv.html"><strong aria-hidden="true">1.10.1.</strong> Provision a PVC on the Block Volume Service</a></li><li class="chapter-item expanded "><a href="gs/pvc-fss.html"><strong aria-hidden="true">1.10.2.</strong> Provision a PVC on the File Storage Service</a></li></ol></li><li class="chapter-item expanded "><a href="gs/customize-worker-node.html"><strong aria-hidden="true">1.11.</strong> Customize worker nodes</a></li><li class="chapter-item expanded "><a href="gs/multi-tenancy.html"><strong aria-hidden="true">1.12.</strong> Multi Tenancy</a></li><li class="chapter-item expanded "><a href="gs/advanced.html"><strong aria-hidden="true">1.13.</strong> Advanced Options</a></li></ol></li><li class="chapter-item expanded "><a href="networking/networking.html"><strong aria-hidden="true">2.</strong> Networking Guide</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="networking/infrastructure.html"><strong aria-hidden="true">2.1.</strong> Default Network Infrastructure</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="networking/calico.html"><strong aria-hidden="true">2.1.1.</strong> Using Calico</a></li><li class="chapter-item expanded "><a href="networking/antrea.html"><strong aria-hidden="true">2.1.2.</strong> Using Antrea</a></li><li class="chapter-item expanded "><a href="networking/ipv6.html"><strong aria-hidden="true">2.1.3.</strong> Using IPv6</a></li></ol></li><li class="chapter-item expanded "><a href="networking/custom-networking.html"><strong aria-hidden="true">2.2.</strong> Custom Networking</a></li><li class="chapter-item expanded "><a href="networking/private-cluster.html"><strong aria-hidden="true">2.3.</strong> Private Cluster</a></li></ol></li><li class="chapter-item expanded "><a href="managed/managedcluster.html"><strong aria-hidden="true">3.</strong> Managed Clusters (OKE)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="managed/virtual-nodes-and-enhanced-clusters.html"><strong aria-hidden="true">3.1.</strong> Virtual Nodes and Enhanced Clusters</a></li><li class="chapter-item expanded "><a href="managed/self-managed-nodes.html"><strong aria-hidden="true">3.2.</strong> Self managed nodes</a></li><li class="chapter-item expanded "><a href="managed/boot-volume-expansion.html"><strong aria-hidden="true">3.3.</strong> Boot volume expansion</a></li><li class="chapter-item expanded "><a href="managed/networking.html"><strong aria-hidden="true">3.4.</strong> Networking customizations</a></li><li class="chapter-item expanded "><a href="managed/features.html"><strong aria-hidden="true">3.5.</strong> Features</a></li></ol></li><li class="chapter-item expanded "><a href="reference/reference.html"><strong aria-hidden="true">4.</strong> Reference</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="reference/api-reference.html"><strong aria-hidden="true">4.1.</strong> API Reference</a></li><li class="chapter-item expanded "><a href="reference/glossary.html"><strong aria-hidden="true">4.2.</strong> Glossary</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Kubernetes Cluster API Provider for Oracle Cloud Infrastructure</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/oracle/cluster-api-provider-oci" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="kubernetes-cluster-api-provider-for-oracle-cloud-infrastructure"><a class="header" href="#kubernetes-cluster-api-provider-for-oracle-cloud-infrastructure">Kubernetes Cluster API Provider for Oracle Cloud Infrastructure</a></h1>
<p><a href="https://goreportcard.com/report/github.com/oracle/cluster-api-provider-oci"><img src="https://goreportcard.com/badge/github.com/oracle/cluster-api-provider-oci" alt="Go Report Card" /></a></p>
<!-- markdownlint-disable MD033 -->
<img src="https://github.com/kubernetes/kubernetes/raw/master/logo/logo.png"  width="100">
<hr />
<p>Kubernetes-native declarative infrastructure for Oracle Cloud Infrastructure (OCI).</p>
<h2 id="what-is-the-cluster-api-provider-for-oci"><a class="header" href="#what-is-the-cluster-api-provider-for-oci">What is the Cluster API Provider for OCI</a></h2>
<p>The <a href="https://github.com/oracle/cluster-api-provider-oci">Cluster API Provider for OCI (CAPOCI)</a> brings declarative, Kubernetes-style APIs to cluster 
creation, configuration and management.</p>
<p>The API itself is shared across multiple cloud providers allowing for true hybrid deployments of Kubernetes.</p>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<ul>
<li>Self-managed and OCI Container Engine for Kubernetes(OKE) cluster support</li>
<li>Manages the bootstrapping of VCNs, gateways, subnets, network security groups</li>
<li>Provides secure and sensible defaults</li>
</ul>
<h2 id="getting-started"><a class="header" href="#getting-started">Getting Started</a></h2>
<ul>
<li><a href="./prerequisites.html">Prerequisites</a>: Set up your OCI tenancy before using CAPOCI.</li>
<li><a href="./gs/overview.html">Deployment process</a>: Choosing your deployment path</li>
<li><a href="./networking/networking.html">Networking</a>: Networking guide</li>
<li>Installation:
<ul>
<li><a href="./gs/install-cluster-api.html">Install Cluster API for OCI</a></li>
<li><a href="./gs/create-workload-cluster.html">Create Workload Cluster</a></li>
<li><a href="./managed/managedcluster.html">Create OKE Cluster</a></li>
</ul>
</li>
</ul>
<h2 id="support-policy"><a class="header" href="#support-policy">Support Policy</a></h2>
<div id="admonition-info" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="introduction.html#admonition-info"></a></p>
</div>
<div>
<p>As the versioning for this project is tied to the versioning of Cluster API, future modifications to this
policy may be made to more closely align with other providers in the Cluster API ecosystem.</p>
</div>
</div>
<h3 id="cluster-api-versions"><a class="header" href="#cluster-api-versions">Cluster API Versions</a></h3>
<p>CAPOCI supports the following Cluster API versions.</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>v1beta1 (v1.0)</th></tr></thead><tbody>
<tr><td>OCI Provider v1beta1 (v0.1)</td><td>✓</td></tr>
</tbody></table>
</div>
<h3 id="kubernetes-versions"><a class="header" href="#kubernetes-versions">Kubernetes versions</a></h3>
<p>CAPOCI provider is able to install and manage the <a href="https://cluster-api.sigs.k8s.io/reference/versions.html#supported-kubernetes-versions">versions of Kubernetes supported by 
Cluster API (CAPI)</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h1>
<h2 id="requirements"><a class="header" href="#requirements">Requirements</a></h2>
<ul>
<li>
<p>An active <a href="https://cloud.oracle.com">Oracle Cloud Infrastructure (OCI) tenancy</a></p>
</li>
<li>
<p>An active user account in the tenancy</p>
</li>
<li>
<p>A supported version of <a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm"><code>oci-cli</code></a></p>
</li>
<li>
<p>A supported version of <a href="https://kubernetes.io/docs/reference/kubectl/kubectl/"><code>kubectl</code></a></p>
</li>
<li>
<p>A <a href="https://github.com/oracle/cluster-api-provider-oci#support-policy">supported version</a> of <a href="https://cluster-api.sigs.k8s.io/clusterctl/overview.html"><code>clusterctl</code></a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-1"><a class="header" href="#getting-started-1">Getting started</a></h1>
<p>This section contains information about enabling and configuring various Oracle Cloud Infrastructure (OCI) resources using the Kubernetes Cluster API Provider for OCI.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-with-kubernetes-cluster-api-provider-for-oracle-cloud-infrastructure"><a class="header" href="#getting-started-with-kubernetes-cluster-api-provider-for-oracle-cloud-infrastructure">Getting started with Kubernetes Cluster API Provider for Oracle Cloud Infrastructure</a></h1>
<p>Before deploying the Cluster API Provider for Oracle Cloud Infrastructure (CAPOCI), you must first configure the 
required Identity and Access Management (IAM) policies:</p>
<p><img src="gs/../images/iam.svg" alt="CAPOCI Installation Process" /></p>
<p>The following deployment options are available:</p>
<ul>
<li><a href="gs/overview.html#getting-started-with-kubernetes-cluster-api-provider-for-oracle-cloud-infracture">Getting started with Kubernetes Cluster API Provider for Oracle Cloud Infrastructure</a>
<ul>
<li><a href="gs/overview.html#setting-up-a-non-production-management-cluster">Setting up a non-production management cluster</a></li>
<li><a href="gs/overview.html#setting-up-a-management-cluster-using-an-initial-bootstrap-cluster">Setting up a management cluster using an initial bootstrap cluster</a></li>
<li><a href="gs/overview.html#setting-up-a-management-cluster-using-oke">Setting up a management cluster using OKE</a></li>
<li><a href="gs/overview.html#setting-up-a-management-cluster-using-a-3rd-party-kubernetes-cluster">Setting up a management cluster using a 3rd party Kubernetes cluster</a></li>
</ul>
</li>
</ul>
<p>The following workflow diagrams provide a high-level overview of each deployment method described above:</p>
<h2 id="setting-up-a-non-production-management-cluster"><a class="header" href="#setting-up-a-non-production-management-cluster">Setting up a non-production management cluster</a></h2>
<p><img src="gs/../images/nonprod.svg" alt="CAPOCI Installation Process" /></p>
<h2 id="setting-up-a-management-cluster-using-an-initial-bootstrap-cluster"><a class="header" href="#setting-up-a-management-cluster-using-an-initial-bootstrap-cluster">Setting up a management cluster using an initial bootstrap cluster</a></h2>
<p><img src="gs/../images/bootstrap.svg" alt="CAPOCI Installation Process" /></p>
<h2 id="setting-up-a-management-cluster-using-oke"><a class="header" href="#setting-up-a-management-cluster-using-oke">Setting up a management cluster using OKE</a></h2>
<p><img src="gs/../images/oke.svg" alt="CAPOCI Installation Process" /></p>
<h2 id="setting-up-a-management-cluster-using-a-3rd-party-kubernetes-cluster"><a class="header" href="#setting-up-a-management-cluster-using-a-3rd-party-kubernetes-cluster">Setting up a management cluster using a 3rd party Kubernetes cluster</a></h2>
<p><img src="gs/../images/3rdparty.svg" alt="CAPOCI Installation Process" /></p>
<p>Complete the following steps in order to install and use CAPOCI:</p>
<ol>
<li>Choose your management cluster. You can use <a href="https://kind.sigs.k8s.io/">kind</a>, <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/home.htm">OKE</a> or any other compliant Kubernetes clusters.</li>
<li><a href="gs/./custom-machine-images.html">Prepare custom machine images</a></li>
<li><a href="gs/./iam.html">Configure users and policies for the management cluster if required</a></li>
<li><a href="gs/./provision-mgmt-cluster.html">Provision a management cluster</a>. You can use <a href="https://kind.sigs.k8s.io/">kind</a>, <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/home.htm">OKE</a> or any other compliant Kubernetes clusters.</li>
<li>Install the necessary tools:
<ul>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm">OCI CLI</a></li>
<li><a href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-clusterctl"><code>clusterctl</code></a></li>
<li><a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/"><code>kubectl</code></a></li>
</ul>
</li>
<li><a href="gs/iam/iam-self-provisioned.html">Configure IAM for the workload cluster</a>.</li>
<li><a href="gs/./install-cluster-api.html">Install Kubernetes Cluster API</a> for Oracle Cloud Infrastructure (CAPOCI) in the <em><strong>management cluster</strong></em>.</li>
<li><a href="gs/./create-workload-cluster.html">Create a workload cluster</a>.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configure-custom-machine-images"><a class="header" href="#configure-custom-machine-images">Configure Custom Machine Images</a></h1>
<p>An image is a template of a virtual hard drive. It determines the operating system and other software for a compute instance. In order to use CAPOCI, you must prepare one or more custom images which have all the necessary Kubernetes components pre-installed. The custom image(s) will then be used to instantiate the Kubernetes nodes.</p>
<h2 id="building-a-custom-image"><a class="header" href="#building-a-custom-image">Building a custom image</a></h2>
<p>To create your own custom image in your Oracle Cloud Infrastructure (OCI) tenancy, navigate to <a href="https://image-builder.sigs.k8s.io/capi/providers/oci.html">The Image Builder Book</a> and follow the instructions for OCI.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configure-user-and-policies"><a class="header" href="#configure-user-and-policies">Configure user and policies</a></h1>
<ul>
<li><a href="gs/iam/iam-oke.html">Configure policies for an OKE cluster</a></li>
<li><a href="gs/iam/iam-self-provisioned.html">Configure policies for a self-provisioned cluster</a></li>
<li><a href="gs/iam/iam-ocid.html">User configuration and OCIDs</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configure-oci-policies-for-an-oracle-container-engine-for-kubernetes-cluster"><a class="header" href="#configure-oci-policies-for-an-oracle-container-engine-for-kubernetes-cluster">Configure OCI policies for an Oracle Container Engine for Kubernetes cluster</a></h1>
<p>These steps are applicable if you intend to run your management cluster using <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/home.htm">Oracle Container Engine for Kubernetes</a> (OKE). They need to be created by a user with admin privileges and are required so you can provision your OKE cluster successfully. If you plan to run your management cluster in <a href="https://kind.sigs.k8s.io/">kind</a> or a non-OKE cluster, you can skip this step.</p>
<ol>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managingusers.htm">Create a user in OCI</a> e.g. <code>iaas_oke_usr</code></li>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managinggroups.htm">Create a group in OCI</a> e.g. <code>iaas_oke_grp</code> and add the user <code>iaas_oke_usr</code> to this group</li>
<li>Create a policy in OCI and add the following policies(Please read <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengpolicyconfig.htm">OKE Policy Configuration Doc</a> for more fine grained policies):
<ul>
<li><code>Allow group iaas_oke_grp to manage dynamic groups</code></li>
<li><code>Allow group iaas_oke_grp to manage virtual-network-family in &lt;compartment&gt;</code></li>
<li><code>Allow group iaas_oke_grp to manage cluster-family in &lt;compartment&gt;</code></li>
<li><code>Allow group iaas_oke_grp to manage instance-family in &lt;compartment&gt;</code></li>
</ul>
</li>
</ol>
<p>where <code>&lt;compartment&gt;</code> is the name of the OCI compartment of the management cluster. Refer to the <a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managingcompartments.htm">OCI documentation</a> if you have not created a compartment yet.</p>
<div id="admonition-warning" class="admonition warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="gs/iam/iam-oke.html#admonition-warning"></a></p>
</div>
<div>
<p>You should not create your management cluster in the root compartment.</p>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configure-policies-for-a-self-provisioned-cluster"><a class="header" href="#configure-policies-for-a-self-provisioned-cluster">Configure policies for a self-provisioned cluster</a></h1>
<p>Although some policies required for Oracle Container Engine for Kubernetes (OKE) and self-provisioned clusters may overlap, we recommend you create another user and group for the principal that will be provisioning the self-provisioned clusters.</p>
<ol>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managingusers.htm">Create a user in OCI</a> e.g. <code>cluster_api_usr</code></li>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managinggroups.htm">Create a group in OCI</a> e.g. <code>cluster_api_grp</code> and add the user <code>cluster_api_usr</code> to this group</li>
<li>Create a policy in OCI and add the following policies:
<ul>
<li><code>Allow group cluster_api_grp to manage virtual-network-family in &lt;compartment&gt;</code></li>
<li><code>Allow group cluster_api_grp to manage load-balancers in &lt;compartment&gt;</code></li>
<li><code>Allow group cluster_api_grp to manage instance-family in &lt;compartment&gt;</code></li>
</ul>
</li>
</ol>
<p>where <code>&lt;compartment&gt;</code> is the name of the OCI compartment of the workload cluster. Your workload compartment may be different from the management compartment. Refer to the <a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/managingcompartments.htm">OCI documentation</a> if you have not created a compartment yet.</p>
<div id="admonition-info" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="gs/iam/iam-self-provisioned.html#admonition-info"></a></p>
</div>
<div>
<p>If you are an administrator and you are experimenting with CAPOCI, you can skip creating the policies.</p>
</div>
</div>
<ol>
<li>Repeat the procedure as for the <code>iaas_oke_usr</code> above to obtain the IAM details.</li>
</ol>
<div id="admonition-warning" class="admonition warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="gs/iam/iam-self-provisioned.html#admonition-warning"></a></p>
</div>
<div>
<p>You should not create your workload cluster in the root compartment.</p>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="user-configuration-and-ocids"><a class="header" href="#user-configuration-and-ocids">User configuration and OCIDs</a></h1>
<ol>
<li>
<p>Login as the <code>iaas_oke_usr</code> in the OCI Console to configure your OCI key. You can either use the OCI console or openssl to <a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#two">generate an API key</a>.</p>
</li>
<li>
<p>Obtain the following details which you will need in order to create your management cluster with OKE:</p>
<ul>
<li><code>&lt;compartment&gt;</code> OCID
<ul>
<li>Navigate to Identity &gt; Compartments</li>
<li>Click on your compartment</li>
<li>Locate OCID on the page and click on <strong>Copy</strong></li>
</ul>
</li>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#five">Tenancy OCID</a></li>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#five">User OCID</a></li>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#four">API key fingerprint</a></li>
</ul>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provision-a-management-cluster"><a class="header" href="#provision-a-management-cluster">Provision a management cluster</a></h1>
<p>Cluster API Provider for Oracle Cloud Infrastructure is installed into an existing Kubernetes cluster, called the <a href="https://cluster-api.sigs.k8s.io/user/concepts.html#management-cluster">management cluster</a>.</p>
<p>You may use <a href="https://kind.sigs.k8s.io/">kind</a> for experimental purposes or for creating a <a href="https://cluster-api.sigs.k8s.io/user/quick-start.html#install-andor-configure-a-kubernetes-cluster">local bootstrap cluster</a> which you will then use to provision a target management cluster.</p>
<ul>
<li><a href="gs/./mgmt/mgmt-kind.html">Create a local management or bootstrap cluster with kind</a></li>
</ul>
<p>For a more durable environment, we recommend using a managed Kubernetes service such as <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/home.htm">Oracle Container Engine for Kubernetes</a> (OKE).</p>
<ul>
<li><a href="gs/./mgmt/mgmt-oke.html">Create a management cluster with OKE</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provision-a-management-cluster-using-kind"><a class="header" href="#provision-a-management-cluster-using-kind">Provision a management cluster using <a href="https://kind.sigs.k8s.io/">kind</a></a></h1>
<ol>
<li>
<p>Create the cluster</p>
<pre><code class="language-shell">  kind create cluster
</code></pre>
</li>
<li>
<p>Configure access</p>
<pre><code class="language-shell">  kubectl config set-context kind-kind
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provision-a-management-cluster-with-oracle-container-engine-for-kubernetes"><a class="header" href="#provision-a-management-cluster-with-oracle-container-engine-for-kubernetes">Provision a management cluster with Oracle Container Engine for Kubernetes</a></h1>
<p>For this release, if you use <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/home.htm">Oracle Container Engine for Kubernetes</a> (OKE) for your management cluster, you will be provisioning a public Kubernetes cluster i.e. its API server must be accessible to <code>kubectl</code>. You can use either use the OCI console to do the provisioning or the <a href="https://github.com/oracle-terraform-modules/terraform-oci-oke">terraform-oci-oke</a> project.</p>
<ol>
<li>
<p>Login to the OCI Console as the <code>iaas_oke_usr</code></p>
</li>
<li>
<p>Search for OKE and select it:</p>
<p><img src="gs/mgmt/../../images/oke_1.png" alt="OKE" /></p>
</li>
<li>
<p>Select the right compartment where you will be creating the OKE Cluster:</p>
<p><img src="gs/mgmt/../../images/oke_2.png" alt="OKE" /></p>
</li>
<li>
<p>Click <strong>Create Cluster</strong>, select <strong>Quick Create</strong> and click <strong>Launch Workflow</strong>:</p>
<p><img src="gs/mgmt/../../images/oke_3.png" alt="OKE" /></p>
</li>
<li>
<p>Name your cluster and ensure you select <strong>Public Endpoint</strong> and choose <strong>Private Workers</strong>:</p>
<p><img src="gs/mgmt/../../images/oke_4.png" alt="OKE" /></p>
</li>
<li>
<p>Click <strong>Next</strong> and <strong>Create Cluster</strong>.</p>
</li>
<li>
<p>When the cluster is ready, set up access to the OKE cluster. You can either use</p>
<ul>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdownloadkubeconfigfile.htm#cloudshelldownload">OCI Cloud Shell</a></li>
<li>or <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdownloadkubeconfigfile.htm#localdownload">your local terminal</a>.</li>
</ul>
</li>
</ol>
<div id="admonition-warning" class="admonition warning">
<div class="admonition-title">
<p>Warning</p>
<p><a class="admonition-anchor-link" href="gs/mgmt/mgmt-oke.html#admonition-warning"></a></p>
</div>
<div>
<p>If you are working with an existing Kubernetes cluster and have an existing <code>kubeconfig</code> in your <code>$HOME/.kube/config</code> directory, running the command to set up local access will add a new cluster context to your existing <code>kubeconfig</code>.</p>
</div>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-cluster-api-provider-for-oracle-cloud-infrastructure"><a class="header" href="#install-cluster-api-provider-for-oracle-cloud-infrastructure">Install Cluster API Provider for Oracle Cloud Infrastructure</a></h1>
<p>If you are not using <a href="https://kind.sigs.k8s.io/">kind</a> for your management cluster, export the <code>KUBECONFIG</code> environment variable to point to the correct Kubeconfig file.</p>
<pre><code class="language-shell">   export KUBECONFIG=/path/to/kubeconfig
</code></pre>
<h2 id="configure-authentication"><a class="header" href="#configure-authentication">Configure authentication</a></h2>
<p>Before installing Cluster API Provider for OCI (CAPOCI), you must first set up your preferred
authentication mechanism using specific environment variables.</p>
<h3 id="user-principal"><a class="header" href="#user-principal">User Principal</a></h3>
<p>If the management cluster is hosted outside OCI, for example a Kind cluster, please configure
user principal using the following parameters. Please refer to the <a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm">doc</a> to generate the required
credentials.</p>
<pre><code class="language-bash">   export OCI_TENANCY_ID=&lt;insert-tenancy-id-here&gt;
   export OCI_USER_ID=&lt;insert-user-ocid-here&gt;
   export OCI_CREDENTIALS_FINGERPRINT=&lt;insert-fingerprint-here&gt;
   export OCI_REGION=&lt;insert-region-here&gt;
   # if Passphrase is present
   export OCI_TENANCY_ID_B64=&quot;$(echo -n &quot;$OCI_TENANCY_ID&quot; | base64 | tr -d '\n')&quot;
   export OCI_CREDENTIALS_FINGERPRINT_B64=&quot;$(echo -n &quot;$OCI_CREDENTIALS_FINGERPRINT&quot; | base64 | tr -d '\n')&quot;
   export OCI_USER_ID_B64=&quot;$(echo -n &quot;$OCI_USER_ID&quot; | base64 | tr -d '\n')&quot;
   export OCI_REGION_B64=&quot;$(echo -n &quot;$OCI_REGION&quot; | base64 | tr -d '\n')&quot;
   export OCI_CREDENTIALS_KEY_B64=$(base64 &lt; &lt;insert-path-to-api-private-key-file-here&gt; | tr -d '\n')
   # if Passphrase is present
   export OCI_CREDENTIALS_PASSPHRASE=&lt;insert-passphrase-here&gt;
   export OCI_CREDENTIALS_PASSPHRASE_B64=&quot;$(echo -n &quot;$OCI_CREDENTIALS_PASSPHRASE&quot; | base64 | tr -d '\n')&quot;
</code></pre>
<h3 id="instance-principal"><a class="header" href="#instance-principal">Instance Principal</a></h3>
<p>If the management cluster is hosted in Oracle Cloud Infrastructure, <a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/callingservicesfrominstances.htm">Instance principals</a> authentication
is recommended. Export the following parameters to use Instance Principals. If Instance Principals are used, the user principal
parameters explained in above section will not be used.</p>
<pre><code class="language-bash">   export USE_INSTANCE_PRINCIPAL=&quot;true&quot;
   export USE_INSTANCE_PRINCIPAL_B64=&quot;$(echo -n &quot;$USE_INSTANCE_PRINCIPAL&quot; | base64 | tr -d '\n')&quot;
</code></pre>
<p>Please ensure the following policies in the dynamic group for CAPOCI to be able to talk to various OCI Services.</p>
<pre><code>allow dynamic-group [your dynamic group name] to manage instance-family in compartment [your compartment name]
allow dynamic-group [your dynamic group name] to manage virtual-network-family in compartment [your compartment name]
allow dynamic-group [your dynamic group name] to manage load-balancers in compartment [your compartment name]
</code></pre>
<h2 id="initialize-management-cluster"><a class="header" href="#initialize-management-cluster">Initialize management cluster</a></h2>
<p>Initialize management cluster and install CAPOCI.</p>
<p>The following command will use the <a href="https://github.com/oracle/cluster-api-provider-oci/releases/latest">latest version</a>:</p>
<pre><code class="language-bash">  clusterctl init --infrastructure oci
</code></pre>
<p>In production, it is recommended to set a specific released version. </p>
<pre><code class="language-bash">  clusterctl init --infrastructure oci:vX.X.X
</code></pre>
<h2 id="capoci-components"><a class="header" href="#capoci-components">CAPOCI Components</a></h2>
<p>When installing CAPOCI, the following components will be installed in the management cluster:</p>
<ol>
<li>A custom resource definition (<code>CRD</code>) for <code>OCICluster</code>, which is a Kubernetes custom resource that represents a workload cluster created in OCI by CAPOCI.</li>
<li>A custom resource definition (<code>CRD</code>) for <code>OCIMachine</code>, which is a Kubernetes custom resource that represents one node in the workload cluster created in OCI by CAPOCI.</li>
<li>Role-based access control resources for a Kubernetes <code>Deployment</code>, <code>ServiceAccount</code>, <code>Role</code>, <code>ClusterRole</code> and <code>ClusterRoleBinding</code></li>
<li>A Kubernetes <code>Secret</code> which will hold OCI credentials</li>
<li>A Kubernetes <code>Deployment</code> with the CAPOCI image - ghcr.io/oracle/cluster-api-oci-controller: <code>&lt;version&gt;</code></li>
</ol>
<p>Please inspect the <code>infrastructure-components.yaml</code> present in the release artifacts to know more.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-a-workload-cluster"><a class="header" href="#create-a-workload-cluster">Create a workload cluster</a></h1>
<h2 id="workload-cluster-templates"><a class="header" href="#workload-cluster-templates">Workload Cluster Templates</a></h2>
<p>Choose one of the available templates for to create your workload clusters from the
<a href="https://github.com/oracle/cluster-api-provider-oci/releases/latest">latest released artifacts</a>. Please note that the templates provided 
are to be considered as references and can be customized further as 
the [CAPOCI API Reference][api-reference]. Each workload cluster template can be
further configured  with the parameters below.</p>
<h2 id="workload-cluster-parameters"><a class="header" href="#workload-cluster-parameters">Workload Cluster Parameters</a></h2>
<p>The following Oracle Cloud Infrastructure (OCI) configuration parameters are available
when creating a workload cluster on OCI using one of our predefined templates:</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>OCI_COMPARTMENT_ID</code></td><td></td><td>The OCID of the compartment in which to create the required compute, storage and network resources.</td></tr>
<tr><td><code>OCI_IMAGE_ID</code></td><td></td><td>The OCID of the image for the kubernetes nodes. This same image is used for both the control plane and the worker nodes.</td></tr>
<tr><td><code>OCI_CONTROL_PLANE_MACHINE_TYPE</code></td><td>VM.Standard.E4.Flex</td><td>The <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm">shape</a> of the Kubernetes control plane machine.</td></tr>
<tr><td><code>OCI_CONTROL_PLANE_MACHINE_TYPE_OCPUS</code></td><td>1</td><td>The number of OCPUs allocated to the control plane instance.</td></tr>
<tr><td><code>OCI_NODE_MACHINE_TYPE</code></td><td>VM.Standard.E4.Flex</td><td>The <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm">shape</a> of the Kubernetes worker machine.</td></tr>
<tr><td><code>OCI_NODE_MACHINE_TYPE_OCPUS</code></td><td>1</td><td>The number of OCPUs allocated to the worker instance.</td></tr>
<tr><td><code>OCI_SSH_KEY</code></td><td></td><td>The public SSH key to be added to the Kubernetes nodes. It can be used to login to the node and troubleshoot failures.</td></tr>
<tr><td><code>OCI_CONTROL_PLANE_PV_TRANSIT_ENCRYPTION</code></td><td>true</td><td>Enables <a href="https://docs.oracle.com/en-us/iaas/Content/File/Tasks/intransitencryption.htm">in-flight Transport Layer Security (TLS) 1.2 encryption</a> of data between control plane nodes and their associated block storage devices.</td></tr>
<tr><td><code>OCI_NODE_PV_TRANSIT_ENCRYPTION</code></td><td>true</td><td>Enables <a href="https://docs.oracle.com/en-us/iaas/Content/File/Tasks/intransitencryption.htm">in-flight Transport Layer Security (TLS) 1.2 encryption</a> of data between worker nodes and their associated block storage devices.</td></tr>
</tbody></table>
</div>
<blockquote>
<p>Note: Only specific <a href="https://docs.oracle.com/en-us/iaas/releasenotes/changes/60d602f5-abb3-4639-aa19-292a5744a808/">bare metal shapes</a>
support in-transit encryption. If an unsupported shape is specified, the deployment will fail completely.</p>
</blockquote>
<blockquote>
<p>Note: Using the predefined templates the machine's memory size is automatically allocated based on the chosen shape 
and OCPU count.</p>
</blockquote>
<p>The following Cluster API parameters are also available:</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>CLUSTER_NAME</code></td><td></td><td>The name of the workload cluster to create.</td></tr>
<tr><td><code>CONTROL_PLANE_MACHINE_COUNT</code></td><td>1</td><td>The number of control plane machines for the workload cluster.</td></tr>
<tr><td><code>KUBERNETES_VERSION</code></td><td></td><td>The Kubernetes version installed on the workload cluster nodes. If this environment variable is not configured, the version must be specified in the <code>.cluster-api/clusterctl.yaml</code> file</td></tr>
<tr><td><code>NAMESPACE</code></td><td></td><td>The namespace for the workload cluster. If not specified, the current namespace is used.</td></tr>
<tr><td><code>POD_CIDR</code></td><td>192.168.0.0/16</td><td>CIDR range of the Kubernetes pod-to-pod network.</td></tr>
<tr><td><code>SERVICE_CIDR</code></td><td>10.128.0.0/12</td><td>CIDR range of the Kubernetes pod-to-services network.</td></tr>
<tr><td><code>NODE_MACHINE_COUNT</code></td><td></td><td>The number of worker machines for the workload cluster.</td></tr>
</tbody></table>
</div>
<h2 id="create-a-new-workload-cluster-on-virtual-instances-using-an-ubuntu-custom-image"><a class="header" href="#create-a-new-workload-cluster-on-virtual-instances-using-an-ubuntu-custom-image">Create a new workload cluster on virtual instances using an Ubuntu custom image</a></h2>
<p>The following command will create a workload cluster comprising a single 
control plane node and single worker node using the default values as specified in the preceding 
<a href="gs/create-workload-cluster.html#workload-cluster-parameters">Workload Cluster Parameters</a> table:</p>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;ubuntu-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
CONTROL_PLANE_MACHINE_COUNT=1 \
KUBERNETES_VERSION=v1.20.10 \
NAMESPACE=default \
NODE_MACHINE_COUNT=1 \
clusterctl generate cluster &lt;cluster-name&gt;\
--from cluster-template.yaml | kubectl apply -f -
</code></pre>
<h2 id="create-a-new-workload-cluster-on-bare-metal-instances-using-an-ubuntu-custom-image"><a class="header" href="#create-a-new-workload-cluster-on-bare-metal-instances-using-an-ubuntu-custom-image">Create a new workload cluster on bare metal instances using an Ubuntu custom image</a></h2>
<p>The following command uses the <code>OCI_CONTROL_PLANE_MACHINE_TYPE</code> and <code>OCI_NODE_MACHINE_TYPE</code> 
parameters to specify bare metal shapes instead of using CAPOCI's default virtual 
instance shape. The <code>OCI_CONTROL_PLANE_PV_TRANSIT_ENCRYPTION</code> and <code>OCI_NODE_PV_TRANSIT_ENCRYPTION</code> 
parameters disable encryption of data in flight between the bare metal instance and the block storage resources.</p>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;ubuntu-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
OCI_CONTROL_PLANE_MACHINE_TYPE=BM.Standard2.52 \
OCI_CONTROL_PLANE_MACHINE_TYPE_OCPUS=52 \
OCI_CONTROL_PLANE_PV_TRANSIT_ENCRYPTION=false \
OCI_NODE_MACHINE_TYPE=BM.Standard2.52 \
OCI_NODE_MACHINE_TYPE_OCPUS=52 \
OCI_NODE_PV_TRANSIT_ENCRYPTION=false \
CONTROL_PLANE_MACHINE_COUNT=1 \
KUBERNETES_VERSION=v1.20.10 \
NAMESPACE=default \
WORKER_MACHINE_COUNT=1 \
clusterctl generate cluster &lt;cluster-name&gt;\
--from cluster-template.yaml| kubectl apply -f -
</code></pre>
<h2 id="create-a-new-workload-cluster-on-virtual-instances-using-an-oracle-linux-custom-image"><a class="header" href="#create-a-new-workload-cluster-on-virtual-instances-using-an-oracle-linux-custom-image">Create a new workload cluster on virtual instances using an Oracle Linux custom image</a></h2>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;oracle-linux-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
CONTROL_PLANE_MACHINE_COUNT=1 \
KUBERNETES_VERSION=v1.20.10 \
NAMESPACE=default \
WORKER_MACHINE_COUNT=1 \
clusterctl generate cluster &lt;cluster-name&gt;\
--from cluster-template-oraclelinux.yaml | kubectl apply -f -
</code></pre>
<h2 id="create-a-workload-cluster-in-an-alternative-region"><a class="header" href="#create-a-workload-cluster-in-an-alternative-region">Create a workload cluster in an alternative region</a></h2>
<p>CAPOCI provides a way to launch and manage your workload cluster in multiple
regions. Choose the <code>cluster-template-alternative-region.yaml</code> template when
creating your workload clusters from the <a href="https://github.com/oracle/cluster-api-provider-oci/releases/latest">latest released artifacts</a>.
Currently, the other templates do not support the ability to change the workload
cluster region. </p>
<p>Each cluster can be further configured with the parameters
defined in <a href="gs/create-workload-cluster.html#workload-cluster-parameters">Workload Cluster Parameters</a> and
additionally with the parameter below.</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>OCI_WORKLOAD_REGION</code></td><td>Configured as <a href="gs/./install-cluster-api.html#configure-authentication"><code>OCI_REGION</code></a></td><td>The <a href="https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm">OCI region</a> in which to launch the workload cluster.</td></tr>
</tbody></table>
</div>
<p>The following example configures the CAPOCI provider to authenticate in
<code>us-phoenix-1</code> and launch a workload cluster in <code>us-sanjose-1</code>.</p>
<blockquote>
<p>Note: Ensure the specified image is available in your chosen region or the launch will fail.</p>
</blockquote>
<p>To configure authentication for management cluster, follow the steps in 
<a href="gs/./install-cluster-api.html#configure-authentication">Configure authentication</a>.</p>
<p>Extend the preceding configuration with the following additional configuration
parameter and initialize the CAPOCI provider.</p>
<pre><code class="language-bash">...
export OCI_REGION=us-phoenix-1
...
   
clusterctl init --infrastructure oci
</code></pre>
<p>Create a new workload cluster in San Jose (<code>us-sanjose-1</code>) by explicitly setting the
<code>OCI_WORKLOAD_REGION</code> environment variable when invoking <code>clusterctl</code>:</p>
<pre><code class="language-bash">OCI_WORKLOAD_REGION=us-sanjose-1 \
OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;in-region-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
CONTROL_PLANE_MACHINE_COUNT=1 \
KUBERNETES_VERSION=v1.20.10 \
NAMESPACE=default \
NODE_MACHINE_COUNT=1 \
clusterctl generate cluster &lt;cluster-name&gt;\
--from cluster-template-alternative-region.yaml | kubectl apply -f -
</code></pre>
<h3 id="access-workload-cluster-kubeconfig"><a class="header" href="#access-workload-cluster-kubeconfig">Access workload cluster Kubeconfig</a></h3>
<p>Execute the following command to list all the workload clusters present:</p>
<pre><code class="language-bash">kubectl get clusters -A
</code></pre>
<p>Execute the following command to access the kubeconfig of a workload cluster:</p>
<pre><code class="language-bash">clusterctl get kubeconfig &lt;cluster-name&gt; -n default &gt; &lt;cluster-name&gt;.kubeconfig
</code></pre>
<h3 id="install-a-cni-provider"><a class="header" href="#install-a-cni-provider">Install a CNI Provider</a></h3>
<p>After creating a workload cluster, a <a href="https://www.cni.dev/">CNI</a> provider must be installed in the workload cluster. Until you install a
a <a href="https://www.cni.dev/">CNI</a> provider, the cluster nodes will not go into the <code>Ready</code> state.</p>
<p>For example, you can install <a href="gs/../networking/calico.html">Calico</a> as follows:</p>
<pre><code class="language-bash">kubectl --kubeconfig=&lt;cluster-name&gt;.kubeconfig \
  apply -f https://docs.projectcalico.org/v3.21/manifests/calico.yaml
</code></pre>
<p>You can use your preferred CNI provider. Currently, the following providers have been tested and verified to work:</p>
<div class="table-wrapper"><table><thead><tr><th>CNI</th><th>CNI Version</th><th>Kubernetes Version</th><th>CAPOCI Version</th></tr></thead><tbody>
<tr><td><a href="gs/../networking/calico.html">Calico</a></td><td>3.21</td><td>1.20.10</td><td>0.1</td></tr>
<tr><td><a href="gs/../networking/antrea.html">Antrea</a></td><td></td><td>1.20.10</td><td>0.1</td></tr>
</tbody></table>
</div>
<p>If you have tested an alternative CNI provider and verified it to work, please send us a PR to add it to the list.</p>
<p>If you have an issue with your alternative CNI provider, please raise an issue on GitHub.</p>
<h3 id="install-oci-cloud-controller-manager-and-csi-in-a-self-provisioned-cluster"><a class="header" href="#install-oci-cloud-controller-manager-and-csi-in-a-self-provisioned-cluster">Install OCI Cloud Controller Manager and CSI in a self-provisioned cluster</a></h3>
<p>By default, the <a href="https://github.com/oracle/oci-cloud-controller-manager">OCI Cloud Controller Manager (CCM)</a> is not installed into a workload cluster. To install the OCI CCM, follow <a href="gs/./install-oci-ccm.html">these instructions</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-a-workload-cluster-with-machinehealthchecks-mhc"><a class="header" href="#create-a-workload-cluster-with-machinehealthchecks-mhc">Create a workload cluster with MachineHealthChecks (MHC)</a></h1>
<p>To better understand MachineHealthChecks please read over <a href="https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking.html">the Cluster-API book</a>
and make sure to read the <a href="https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking.html#limitations-and-caveats-of-a-machinehealthcheck">limitations</a> sections.</p>
<h2 id="create-a-new-workload-cluster-with-mhc"><a class="header" href="#create-a-new-workload-cluster-with-mhc">Create a new workload cluster with MHC</a></h2>
<p>In the project's code repository we provide an <a href="https://github.com/oracle/cluster-api-provider-oci/blob/main/templates/cluster-template-healcheck.yaml">example template</a> that sets up two MachineHealthChecks
at workload creation time. The example sets up two MHCs to allow differing remediation values:</p>
<ul>
<li><code>control-plane-unhealthy-5m</code> setups a health check for the control plane machines</li>
<li><code>md-unhealthy-5m</code> sets up a health check for the workload machines</li>
</ul>
<blockquote>
<p>NOTE: As a part of the example template the MHCs will start remediating nodes that are <code>not ready</code> after 10 minutes.
In order prevent this side effect make sure to <a href="gs/../gs/create-workload-cluster.html#install-a-cni-provider">install your CNI</a> once the API is available.
This will move the machines into a <code>Ready</code> state.</p>
</blockquote>
<h2 id="add-mhc-to-existing-workload-cluster"><a class="header" href="#add-mhc-to-existing-workload-cluster">Add MHC to existing workload cluster</a></h2>
<p>Another approach is to install MHC after the cluster is up and healthy (aka Day-2 Operation). This can prevent
machine remediation while setting up the cluster.</p>
<p>Adding the MHC to either control-plane or machine is a multistep process. The steps are run on specific clusters
(e.g. management cluster, workload cluster):</p>
<ol>
<li>Update the spec for future instances (management cluster)</li>
<li>Add label to existing nodes (workload cluster)</li>
<li>Add the MHC (management cluster)</li>
</ol>
<h3 id="add-control-plane-mhc"><a class="header" href="#add-control-plane-mhc">Add control-plane MHC</a></h3>
<h4 id="update-control-plane-spec"><a class="header" href="#update-control-plane-spec">Update control plane spec</a></h4>
<p>We need to add the <code>controlplane.remediation</code> label to the <code>KubeadmControlPlane</code>.</p>
<p>Create a file named <code>control-plane-patch.yaml</code> that has this content:</p>
<pre><code class="language-yaml">spec:
  machineTemplate:
    metadata:
      labels:
        controlplane.remediation: &quot;&quot;
</code></pre>
<p>Then on the management cluster run
<code>kubectl patch KubeadmControlPlane &lt;your-cluster-name&gt;-control-plane --patch-file control-plane-patch.yaml --type=merge</code>.</p>
<h4 id="add-label-to-existing-nodes"><a class="header" href="#add-label-to-existing-nodes">Add label to existing nodes</a></h4>
<p>Then on the workload cluster add the new label to any existing control-plane node(s)
<code>kubectl label node &lt;control-plane-name&gt; controlplane.remediation=&quot;&quot;</code>. This will prevent the <code>KubeadmControlPlane</code> provisioning
new nodes once the MHC is deployed.</p>
<h4 id="add-the-mhc"><a class="header" href="#add-the-mhc">Add the MHC</a></h4>
<p>Finally, create a file named <code>control-plane-mhc.yaml</code> that has this content: </p>
<pre><code class="language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: &quot;&lt;your-cluster-name&gt;-control-plane-unhealthy-5m&quot;
spec:
  clusterName: &quot;&lt;your-cluster-name&gt;&quot;
  maxUnhealthy: 100%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      controlplane.remediation: &quot;&quot;
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 300s
    - type: Ready
      status: &quot;False&quot;
      timeout: 300s
</code></pre>
<p>Then on the management cluster run <code>kubectl apply -f control-plane-mhc.yaml</code>.</p>
<p>Then run <code>kubectl get machinehealthchecks</code> to check your MachineHealthCheck sees the expected machines.</p>
<h3 id="add-machine-mhc"><a class="header" href="#add-machine-mhc">Add machine MHC</a></h3>
<h4 id="update-machine-spec"><a class="header" href="#update-machine-spec">Update machine spec</a></h4>
<p>We need to add the <code>machine.remediation</code> label to the <code>MachineDeployment</code>.</p>
<p>Create a file named <code>machine-patch.yaml</code> that has this content:</p>
<pre><code class="language-yaml">spec:
  template:
    metadata:
      labels:
        machine.remediation: &quot;&quot;
</code></pre>
<p>Then on the management cluster run
<code>kubectl patch MachineDeployment oci-cluster-stage-md-0 --patch-file machine-patch.yaml --type=merge</code>.</p>
<h4 id="add-label-to-existing-nodes-1"><a class="header" href="#add-label-to-existing-nodes-1">Add label to existing nodes</a></h4>
<p>Then on the workload cluster add the new label to any existing control-plane node(s)
<code>kubectl label node &lt;machine-name&gt; machine.remediation=&quot;&quot;</code>. This will prevent the <code>MachineDeployment</code> provisioning
new nodes once the MHC is deployed.</p>
<h4 id="add-the-mhc-1"><a class="header" href="#add-the-mhc-1">Add the MHC</a></h4>
<p>Finally, create a file named <code>machine-mhc.yaml</code> that has this content:</p>
<pre><code class="language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: &quot;&lt;your-cluster-name&gt;-stage-md-unhealthy-5m&quot;
spec:
  clusterName: &quot;oci-cluster-stage&quot;
  maxUnhealthy: 100%
  nodeStartupTimeout: 10m
  selector:
    matchLabels:
      machine.remediation: &quot;&quot;
  unhealthyConditions:
    - type: Ready
      status: Unknown
      timeout: 300s
    - type: Ready
      status: &quot;False&quot;
      timeout: 300s
</code></pre>
<p>Then on the management cluster run <code>kubectl apply -f machine-mhc.yaml</code>.</p>
<p>Then run <code>kubectl get machinehealthchecks</code> to check your MachineHealthCheck sees the expected machines.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-a-gpu-workload-cluster"><a class="header" href="#create-a-gpu-workload-cluster">Create a GPU workload cluster</a></h1>
<h2 id="accessing-gpu-shapes"><a class="header" href="#accessing-gpu-shapes">Accessing GPU Shapes</a></h2>
<p>Some shapes are limited to specific regions and specific Availability Domains (AD).
In order to make sure the workload cluster comes up check the region and AD for 
shape availability.</p>
<h3 id="check-shape-availability"><a class="header" href="#check-shape-availability">Check shape availability</a></h3>
<p>Make sure the <a href="https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/cliinstall.htm">OCI CLI</a> is installed. Then set the AD information if using 
muti-AD regions.</p>
<blockquote>
<p>NOTE: Use the <a href="https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm">OCI Regions and Availability Domains</a> page to figure out which 
regions have multiple ADs.</p>
</blockquote>
<pre><code class="language-bash">oci iam availability-domain list --compartment-id=&lt;your compartment&gt; --region=&lt;region&gt;
</code></pre>
<p>Using the AD <code>name</code> from the output start searching for GPU shape availability.</p>
<pre><code class="language-bash">oci compute shape list --compartment-id=&lt;your compartment&gt; --profile=DEFAULT --region=us-ashburn-1 --availability-domain=&lt;your AD ID&gt; | grep GPU
 
&quot;shape-name&quot;: &quot;BM.GPU3.8&quot;
&quot;shape-name&quot;: &quot;BM.GPU4.8&quot;
&quot;shape-name&quot;: &quot;VM.GPU3.1&quot;
&quot;shape&quot;: &quot;VM.GPU2.1&quot;
</code></pre>
<blockquote>
<p>NOTE: If the output is empty then the compartment for that region/AD doesn't have GPU shapes.
If you are unable to locate any shapes you may need to submit a
<a href="https://docs.oracle.com/en-us/iaas/Content/General/Concepts/servicelimits.htm#computelimits">service limit increase request</a></p>
</blockquote>
<h2 id="create-a-new-gpu-workload-cluster-an-ubuntu-custom-image"><a class="header" href="#create-a-new-gpu-workload-cluster-an-ubuntu-custom-image">Create a new GPU workload cluster an Ubuntu custom image</a></h2>
<blockquote>
<p>NOTE: Nvidia GPU drivers aren't supported for Oracle Linux at this time. Ubuntu is currently
the only supported OS.</p>
</blockquote>
<p>When launching a multi-AD region shapes are likely be limited to a specific AD (example: <code>US-ASHBURN-AD-2</code>).
To make sure the cluster comes up without issue specifically target just that AD for the GPU worker nodes.
To do that modify the released version of the <code>cluster-template-failure-domain-spread.yaml</code> template.</p>
<p>Download the <a href="https://github.com/oracle/cluster-api-provider-oci/releases">latest <code>cluster-template-failure-domain-spread.yaml</code></a> file and save it as
<code>cluster-template-gpu.yaml</code>.</p>
<p>Make sure the modified template has only the <code>MachineDeployment</code> section(s) where there is GPU
availability and remove all the others. See <a href="gs/create-gpu-workload-cluster.html#example-yaml-file">the full example file</a>
that targets only AD 2 (OCI calls them Availability Domains while Cluster-API calls them Failure Domains).</p>
<h3 id="virtual-instances"><a class="header" href="#virtual-instances">Virtual instances</a></h3>
<p>The following command will create a workload cluster comprising a single 
control plane node and single GPU worker node using the default values as specified in the preceding 
<a href="gs/../gs/create-workload-cluster.html#workload-cluster-parameters">Workload Cluster Parameters</a> table:</p>
<blockquote>
<p>NOTE: The <code>OCI_NODE_MACHINE_TYPE_OCPUS</code> must match the OPCU count of the GPU shape.
See the <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm">Compute Shapes</a> page to get the OCPU count for the specific shape.</p>
</blockquote>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;ubuntu-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
NODE_MACHINE_COUNT=1 \
OCI_NODE_MACHINE_TYPE=VM.GPU3.1 \
OCI_NODE_MACHINE_TYPE_OCPUS=6 \
OCI_CONTROL_PLANE_MACHINE_TYPE_OCPUS=1 \
OCI_CONTROL_PLANE_MACHINE_TYPE=VM.Standard3.Flex \
CONTROL_PLANE_MACHINE_COUNT=1 \
OCI_SHAPE_MEMORY_IN_GBS= \
KUBERNETES_VERSION=v1.24.4 \
clusterctl generate cluster &lt;cluster-name&gt; \
--target-namespace default \
--from cluster-template-gpu.yaml | kubectl apply -f -
</code></pre>
<h3 id="bare-metal-instances"><a class="header" href="#bare-metal-instances">Bare metal instances</a></h3>
<p>The following command uses the <code>OCI_CONTROL_PLANE_MACHINE_TYPE</code> and <code>OCI_NODE_MACHINE_TYPE</code> 
parameters to specify bare metal shapes instead of using CAPOCI's default virtual 
instance shape. The <code>OCI_CONTROL_PLANE_PV_TRANSIT_ENCRYPTION</code> and <code>OCI_NODE_PV_TRANSIT_ENCRYPTION</code> 
parameters disable encryption of data in flight between the bare metal instance and the block storage resources.</p>
<blockquote>
<p>NOTE: The <code>OCI_NODE_MACHINE_TYPE_OCPUS</code> must match the OPCU count of the GPU shape.
See the <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm">Compute Shapes</a> page to get the OCPU count for the specific shape.</p>
</blockquote>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;ubuntu-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
OCI_NODE_MACHINE_TYPE=BM.GPU3.8 \
OCI_NODE_MACHINE_TYPE_OCPUS=52 \
OCI_NODE_PV_TRANSIT_ENCRYPTION=false \
OCI_CONTROL_PLANE_MACHINE_TYPE=VM.Standard3.Flex \
CONTROL_PLANE_MACHINE_COUNT=1 \
OCI_SHAPE_MEMORY_IN_GBS= \
KUBERNETES_VERSION=v1.24.4 \
clusterctl generate cluster &lt;cluster-name&gt; \
--target-namespace default \
--from cluster-template-gpu.yaml | kubectl apply -f -
</code></pre>
<h3 id="access-workload-cluster-kubeconfig-1"><a class="header" href="#access-workload-cluster-kubeconfig-1">Access workload cluster Kubeconfig</a></h3>
<p>Execute the following command to list all the workload clusters present:</p>
<pre><code class="language-bash">kubectl get clusters -A
</code></pre>
<p>Execute the following command to access the kubeconfig of a workload cluster:</p>
<pre><code class="language-bash">clusterctl get kubeconfig &lt;cluster-name&gt; -n default &gt; &lt;cluster-name&gt;.kubeconfig
</code></pre>
<h3 id="install-a-cni-provider-oci-cloud-controller-manager-and-csi-in-a-self-provisioned-cluster"><a class="header" href="#install-a-cni-provider-oci-cloud-controller-manager-and-csi-in-a-self-provisioned-cluster">Install a CNI Provider, OCI Cloud Controller Manager and CSI in a self-provisioned cluster</a></h3>
<p>To provision the CNI and Cloud Controller Manager follow the <a href="gs/../gs/create-workload-cluster.html#install-a-cni-provider">Install a CNI Provider</a> 
and the <a href="gs/../gs/create-workload-cluster.html#install-oci-cloud-controller-manager-and-csi-in-a-self-provisioned-cluster">Install OCI Cloud Controller Manager</a> sections.</p>
<h3 id="install-nvidia-gpu-operator"><a class="header" href="#install-nvidia-gpu-operator">Install Nvidia GPU Operator</a></h3>
<p>Setup the worker instances to use the GPUs install the <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/overview.html">Nvidia GPU Operator</a>. </p>
<p>For the most up-to-date install instructions see the <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/getting-started.html#install-nvidia-gpu-operator">official install instructions</a>. They
layout how to install the <a href="https://helm.sh/docs/intro/install/">Helm tool</a> and how to setup the Nvidia helm repo. </p>
<p>With Helm setup you can now install the GPU-Operator</p>
<pre><code class="language-bash">helm install --wait --generate-name \
     -n gpu-operator --create-namespace \
     nvidia/gpu-operator
</code></pre>
<p>The pods will take a while to come up but you can check the status:</p>
<pre><code class="language-bash">kubectl --&lt;cluster-name&gt;.kubeconf get pods -n gpu-operator
</code></pre>
<h3 id="test-gpu-on-worker-node"><a class="header" href="#test-gpu-on-worker-node">Test GPU on worker node</a></h3>
<p>Once all of the GPU-Operator pods are <code>running</code> or <code>completed</code> deploy the test pod:</p>
<pre><code class="language-bash">cat &lt;&lt;EOF | kubectl --kubeconfig=&lt;cluster-name&gt;.kubeconf apply -f -
apiVersion: v1
kind: Pod
metadata:
name: cuda-vector-add
spec:
restartPolicy: OnFailure
containers:
- name: cuda-vector-add
# https://github.com/kubernetes/kubernetes/blob/v1.7.11/test/images/nvidia-cuda/Dockerfile
image: &quot;registry.k8s.io/cuda-vector-add:v0.1&quot;
resources:
limits:
nvidia.com/gpu: 1 # requesting 1 GPU
EOF
</code></pre>
<p>Then check the output logs of the <code>cuda-vector-add</code> test pod:</p>
<pre><code class="language-bash">kubectl --kubeconfig=&lt;cluster-name&gt;.kubeconf logs cuda-vector-add -n default
 
[Vector addition of 50000 elements]
Copy input data from the host memory to the CUDA device
CUDA kernel launch with 196 blocks of 256 threads
Copy output data from the CUDA device to the host memory
Test PASSED
Done
</code></pre>
<h2 id="example-yaml-file"><a class="header" href="#example-yaml-file">Example yaml file</a></h2>
<p>This is an example file using a modified version of <code>cluster-template-failure-domain-spread.yaml</code> 
to target AD 2 (example: <code>US-ASHBURN-AD-2</code>). </p>
<pre><code class="language-yaml">apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: &quot;${CLUSTER_NAME}&quot;
  name: &quot;${CLUSTER_NAME}&quot;
  namespace: &quot;${NAMESPACE}&quot;
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - ${POD_CIDR:=&quot;192.168.0.0/16&quot;}
    serviceDomain: ${SERVICE_DOMAIN:=&quot;cluster.local&quot;}
    services:
      cidrBlocks:
        - ${SERVICE_CIDR:=&quot;10.128.0.0/12&quot;}
  infrastructureRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: OCICluster
    name: &quot;${CLUSTER_NAME}&quot;
    namespace: &quot;${NAMESPACE}&quot;
  controlPlaneRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta1
    kind: KubeadmControlPlane
    name: &quot;${CLUSTER_NAME}-control-plane&quot;
    namespace: &quot;${NAMESPACE}&quot;
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: &quot;${CLUSTER_NAME}&quot;
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
---
kind: KubeadmControlPlane
apiVersion: controlplane.cluster.x-k8s.io/v1beta1
metadata:
  name: &quot;${CLUSTER_NAME}-control-plane&quot;
  namespace: &quot;${NAMESPACE}&quot;
spec:
  version: &quot;${KUBERNETES_VERSION}&quot;
  replicas: ${CONTROL_PLANE_MACHINE_COUNT}
  machineTemplate:
    infrastructureRef:
      kind: OCIMachineTemplate
      apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
      name: &quot;${CLUSTER_NAME}-control-plane&quot;
      namespace: &quot;${NAMESPACE}&quot;
  kubeadmConfigSpec:
    clusterConfiguration:
      kubernetesVersion: ${KUBERNETES_VERSION}
      apiServer:
        certSANs: [localhost, 127.0.0.1]
      dns: {}
      etcd: {}
      networking: {}
      scheduler: {}
    initConfiguration:
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: oci://{{ ds[&quot;id&quot;] }}
    joinConfiguration:
      discovery: {}
      nodeRegistration:
        criSocket: /var/run/containerd/containerd.sock
        kubeletExtraArgs:
          cloud-provider: external
          provider-id: oci://{{ ds[&quot;id&quot;] }}
---
kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
metadata:
  name: &quot;${CLUSTER_NAME}-control-plane&quot;
spec:
  template:
    spec:
      imageId: &quot;${OCI_IMAGE_ID}&quot;
      compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
      shape: &quot;${OCI_CONTROL_PLANE_MACHINE_TYPE=VM.Standard.E4.Flex}&quot;
      shapeConfig:
        ocpus: &quot;${OCI_CONTROL_PLANE_MACHINE_TYPE_OCPUS=1}&quot;
      metadata:
        ssh_authorized_keys: &quot;${OCI_SSH_KEY}&quot;
      isPvEncryptionInTransitEnabled: ${OCI_CONTROL_PLANE_PV_TRANSIT_ENCRYPTION=true}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIMachineTemplate
metadata:
  name: &quot;${CLUSTER_NAME}-md&quot;
spec:
  template:
    spec:
      imageId: &quot;${OCI_IMAGE_ID}&quot;
      compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
      shape: &quot;${OCI_NODE_MACHINE_TYPE=VM.Standard.E4.Flex}&quot;
      shapeConfig:
        ocpus: &quot;${OCI_NODE_MACHINE_TYPE_OCPUS=1}&quot;
      metadata:
        ssh_authorized_keys: &quot;${OCI_SSH_KEY}&quot;
      isPvEncryptionInTransitEnabled: ${OCI_NODE_PV_TRANSIT_ENCRYPTION=true}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha4
kind: KubeadmConfigTemplate
metadata:
  name: &quot;${CLUSTER_NAME}-md&quot;
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            provider-id: oci://{{ ds[&quot;id&quot;] }}
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: &quot;${CLUSTER_NAME}-fd-2-md-0&quot;
spec:
  clusterName: &quot;${CLUSTER_NAME}&quot;
  replicas: ${NODE_MACHINE_COUNT}
  selector:
    matchLabels:
  template:
    spec:
      clusterName: &quot;${CLUSTER_NAME}&quot;
      version: &quot;${KUBERNETES_VERSION}&quot;
      bootstrap:
        configRef:
          name: &quot;${CLUSTER_NAME}-md&quot;
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: &quot;${CLUSTER_NAME}-md&quot;
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: OCIMachineTemplate
      # Cluster-API calls them Failure Domains while OCI calls them Availability Domains
      # In the example this would be targeting US-ASHBURN-AD-2
      failureDomain: &quot;2&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-a-windows-workload-cluster"><a class="header" href="#create-a-windows-workload-cluster">Create a Windows workload cluster</a></h1>
<h2 id="overview"><a class="header" href="#overview">Overview</a></h2>
<p>CAPOCI enables users to create and manage Windows workload clusters in Oracle Cloud Infrastructure (OCI).
This means that the <a href="https://kubernetes.io/docs/concepts/overview/components/#control-plane-components">Kubernetes Control Plane</a> will be Linux and the nodes will be Windows.
First, users build the <a href="https://image-builder.sigs.k8s.io/capi/providers/oci.html#building-a-windows-image">Windows image using image-builder</a>, then use the Windows flavor
template from the <a href="https://github.com/oracle/cluster-api-provider-oci/releases/latest">latest release</a>. Finally, install the <a href="gs/create-windows-workload-cluster.html#install-calico-cni-provider-and-oci-cloud-controller-manager">Calico CNI Provider
and OCI Cloud Controller Manager</a>. </p>
<h2 id="known-limitations"><a class="header" href="#known-limitations">Known Limitations</a></h2>
<p>The Windows workload cluster has known limitations:</p>
<ul>
<li>Limited to <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm#bm-standard">Standard Bare Metal (BM) shapes</a></li>
<li>Limited to OCI provided platform images. See <a href="https://image-builder.sigs.k8s.io/capi/providers/oci.html#building-a-windows-image">image-build documentation</a> for more details</li>
<li>Custom image MUST be built using the same shape of Bare Metal the worker nodes will run</li>
<li>CNI provider support is <a href="https://docs.tigera.io/calico/3.24/getting-started/kubernetes/windows-calico/">Calico in VXLAN mode</a></li>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/GSG/Tasks/addingstorage.htm">Block volumes</a> are not currently supported</li>
<li>Bring Your Own License (BYOL) is not supported</li>
<li>See <a href="https://docs.tigera.io/calico/3.24/getting-started/kubernetes/windows-calico/limitations">Calico windows docs</a> for their limitations </li>
</ul>
<h2 id="licensing"><a class="header" href="#licensing">Licensing</a></h2>
<p>BYOL is currently not supported using CAPOCI. For more info on Windows Licensing
see the <a href="https://www.oracle.com/cloud/compute/faq/#category-windows">Compute FAQ documentation</a>.</p>
<h2 id="build-windows-image"><a class="header" href="#build-windows-image">Build Windows image</a></h2>
<blockquote>
<p>NOTE: It is recommended to <a href="gs/create-windows-workload-cluster.html#check-shape-availability">check shape availability</a> before building image(s)</p>
</blockquote>
<p>In order to launch Windows instances for the cluster a Windows image, <a href="https://image-builder.sigs.k8s.io/capi/providers/oci.html#building-a-windows-image">using image-builder</a>,
will need to be built. It is <strong>important</strong> to make sure the same shape is used to build and launch the instance.</p>
<p>Example: If a <code>BM.Standard2.52</code> is used to build then the <code>OCI_NODE_MACHINE_TYPE</code> MUST
be <code>BM.Standard2.52</code></p>
<h2 id="check-shape-availability-1"><a class="header" href="#check-shape-availability-1">Check shape availability</a></h2>
<p>Make sure the <a href="https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/cliinstall.htm">OCI CLI</a> is installed. Then set the AD information if using
muti-AD regions.</p>
<blockquote>
<p>NOTE: Use the <a href="https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm">OCI Regions and Availability Domains</a> page to figure out which
regions have multiple ADs.</p>
</blockquote>
<pre><code class="language-bash">oci iam availability-domain list --compartment-id=&lt;your compartment&gt; --region=&lt;region&gt;
</code></pre>
<p>Using the AD <code>name</code> from the output above start searching for BM shape availability.</p>
<pre><code class="language-bash">oci compute shape list --compartment-id=&lt;your compartment&gt; --profile=DEFAULT --region=us-ashburn-1 --availability-domain=&lt;your AD ID&gt; | grep BM
 
&quot;shape&quot;: &quot;BM.Standard.E3.128&quot;
&quot;shape-name&quot;: &quot;BM.Standard2.52&quot;
&quot;shape-name&quot;: &quot;BM.Standard.E3.128&quot;
&quot;shape&quot;: &quot;BM.Standard.E2.64&quot;
&quot;shape-name&quot;: &quot;BM.Standard2.52&quot;
&quot;shape-name&quot;: &quot;BM.Standard3.64&quot;
&quot;shape&quot;: &quot;BM.Standard1.36&quot;
</code></pre>
<blockquote>
<p>NOTE: If the output is empty then the compartment for that region/AD doesn't have BM shapes.
If you are unable to locate any shapes you may need to submit a
<a href="https://docs.oracle.com/en-us/iaas/Content/General/Concepts/servicelimits.htm#computelimits">service limit increase request</a></p>
</blockquote>
<h2 id="create-a-new-windows-workload-cluster"><a class="header" href="#create-a-new-windows-workload-cluster">Create a new Windows workload cluster</a></h2>
<p>It is recommended to have the following guides handy:</p>
<ul>
<li><a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/windows/">Windows Cluster Debugging</a></li>
<li><a href="https://learn.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/common-problems">Windows Container Debugging</a></li>
</ul>
<p>When using <code>clusterctl</code> to generate the cluster use the <code>windows-calico</code> example flavor. </p>
<p>The following command uses the <code>OCI_CONTROL_PLANE_MACHINE_TYPE</code> and <code>OCI_NODE_MACHINE_TYPE</code>
parameters to specify bare metal shapes instead of using CAPOCI's default virtual
instance shape. The <code>OCI_CONTROL_PLANE_PV_TRANSIT_ENCRYPTION</code> and <code>OCI_NODE_PV_TRANSIT_ENCRYPTION</code>
parameters disable encryption of data in flight between the bare metal instance and the block storage resources.</p>
<blockquote>
<p>NOTE: The <code>OCI_NODE_MACHINE_TYPE_OCPUS</code> must match the OPCU count of the BM shape.
See the <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/computeshapes.htm#bm-standard">Compute Shapes</a> page to get the OCPU count for the specific shape.</p>
</blockquote>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_CONTROL_PLANE_IMAGE_ID=&lt;linux-custom-image-id&gt; \
OCI_NODE_IMAGE_ID=&lt;windows-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
NODE_MACHINE_COUNT=1 \
OCI_NODE_MACHINE_TYPE=BM.Standard.E4.128 \
OCI_NODE_MACHINE_TYPE_OCPUS=128 \
OCI_NODE_PV_TRANSIT_ENCRYPTION=false \
OCI_CONTROL_PLANE_MACHINE_TYPE_OCPUS=3 \
OCI_CONTROL_PLANE_MACHINE_TYPE=VM.Standard3.Flex \
CONTROL_PLANE_MACHINE_COUNT=3 \
OCI_SHAPE_MEMORY_IN_GBS= \
KUBERNETES_VERSION=&lt;k8s-version&gt; \
clusterctl generate cluster &lt;cluster-name&gt; \
--target-namespace default \
--flavor windows-calico | kubectl apply -f -
</code></pre>
<h3 id="access-workload-cluster-kubeconfig-2"><a class="header" href="#access-workload-cluster-kubeconfig-2">Access workload cluster Kubeconfig</a></h3>
<p>Execute the following command to list all the workload clusters present:</p>
<pre><code class="language-bash">kubectl get clusters -A
</code></pre>
<p>Execute the following command to access the kubeconfig of a workload cluster:</p>
<pre><code class="language-bash">clusterctl get kubeconfig &lt;cluster-name&gt; -n default &gt; &lt;cluster-name&gt;.kubeconfig
</code></pre>
<h3 id="install-calico-cni-provider-and-oci-cloud-controller-manager"><a class="header" href="#install-calico-cni-provider-and-oci-cloud-controller-manager">Install Calico CNI Provider and OCI Cloud Controller Manager</a></h3>
<h4 id="install-calico"><a class="header" href="#install-calico">Install Calico</a></h4>
<p>The <a href="https://docs.tigera.io/calico/3.24/getting-started/kubernetes/windows-calico/">Calico for Windows</a> getting started guide should be read for better understand of the CNI on Windows.
It is recommended to have the following guides handy:</p>
<ul>
<li><a href="https://docs.tigera.io/calico/3.24/getting-started/kubernetes/windows-calico/troubleshoot">Windows Calico Troubleshooting</a></li>
</ul>
<h5 id="the-steps-to-follow"><a class="header" href="#the-steps-to-follow">The steps to follow:</a></h5>
<p><strong>On the management cluster</strong></p>
<ol>
<li>Run 
<pre><code>kubectl get OCICluster &lt;cluster-name&gt; -o jsonpath='{.spec.controlPlaneEndpoint.host}'
</code></pre>
to get the <code>KUBERNETES_SERVICE_HOST</code> info that will be used in later steps</li>
</ol>
<p><strong>On the workload cluster</strong></p>
<ol>
<li>Download the <a href="https://github.com/projectcalico/calico/releases/download/v3.24.5/release-v3.24.5.tgz">v3.24.5 calico release</a> 
<pre><code>curl -L https://github.com/projectcalico/calico/releases/download/v3.24.5/release-v3.24.5.tgz -o calico-v3.24.5.tgz
</code></pre>
</li>
<li>Uncompress the downloaded file and locate the <code>calico-vxlan.yaml</code>, <code>calico-windows-vxlan.yaml</code> and <code>windows-kube-proxy.yaml</code>
files in the <code>manifests</code> dir</li>
<li>Edit the <code>calico-vxlan.yaml</code> and modify the follow variables to allow Calico running on the nodes use VXLAN
<ul>
<li><code>CALICO_IPV4POOL_IPIP</code> - set to <code>&quot;Never&quot;</code></li>
<li><code>CALICO_IPV4POOL_VXLAN</code> - set to <code>&quot;Always&quot;</code></li>
</ul>
</li>
<li>
<pre><code>kubectl apply -f calico-vxlan.yaml
</code></pre>
</li>
<li>Wait for the IPAMConfig to be loaded</li>
<li>
<pre><code>kubectl patch IPAMConfig default --type merge --patch='{&quot;spec&quot;: {&quot;strictAffinity&quot;: true}}'
</code></pre>
</li>
<li>Edit the <code>calico-windows-vxlan.yaml</code> and modify the follow variables to allow Calico running on the nodes to talk
to the Kubernetes control plane
<ul>
<li><code>KUBERNETES_SERVICE_HOST</code> - the IP address from the management cluster step</li>
<li><code>KUBERNETES_SERVICE_PORT</code>- the port from step the management cluster step</li>
<li><code>K8S_SERVICE_CIDR</code> - The service CIDR set in the cluster template</li>
<li><code>DNS_NAME_SERVERS</code> - the IP address from dns service
<pre><code>kubectl  get svc kube-dns -n kube-system  -o jsonpath='{.spec.clusterIP}'
</code></pre>
</li>
<li>Change the namespace from <code>calico-system</code> to <code>kube-system</code></li>
<li>add the following <code>env</code> to the container named <code>node</code>
<pre><code class="language-yaml">- name: VXLAN_ADAPTER
  value: &quot;Ethernet 2&quot;
</code></pre>
</li>
</ul>
</li>
<li>
<pre><code>kubectl apply -f calico-windows-vxlan.yaml
</code></pre>
(it takes a bit for this to pass livenessprobe)</li>
<li>Edit the <code>windows-kube-proxy.yaml</code> 
<ul>
<li>update the <code>kube-proxy</code> container environment variable <code>K8S_VERSION</code> to the version of kubernetes you are deploying</li>
<li>update the <code>image</code> version for the container named <code>kube-proxy</code> and make sure to set the
correct <a href="https://hub.docker.com/_/microsoft-windows-nanoserver">windows nanoserver version</a> example: <code>ltsc2019</code></li>
</ul>
</li>
<li>
<pre><code>kubectl apply -f windows-kube-proxy.yaml
</code></pre>
</li>
</ol>
<h4 id="install-oci-cloud-controller-manager"><a class="header" href="#install-oci-cloud-controller-manager">Install OCI Cloud Controller Manager</a></h4>
<p>By default, the <a href="https://github.com/oracle/oci-cloud-controller-manager">OCI Cloud Controller Manager (CCM)</a> is not installed into a workload cluster. To install the OCI CCM, follow <a href="gs/./install-oci-ccm.html">these instructions</a>.</p>
<h3 id="scheduling-windows-containers"><a class="header" href="#scheduling-windows-containers">Scheduling Windows containers</a></h3>
<p>With the cluster in a ready state and CCM installed, an <a href="https://kubernetes.io/docs/concepts/windows/user-guide/">example deployment</a>
can be used to test that pods are scheduled. Accessing the deployed pods and using <code>nslookup</code> you can 
<a href="https://kubernetes.io/docs/tutorials/services/connect-applications-service/#dns">test that the cluster DNS is working</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="create-workload-templates-for-oracle-cloud-infrastructure"><a class="header" href="#create-workload-templates-for-oracle-cloud-infrastructure">Create Workload Templates for Oracle Cloud Infrastructure</a></h1>
<p>You can create workload clusters based on template files or you can also save the templates in <code>ConfigMaps</code> that you can then reuse.</p>
<h2 id="creating-cluster-templates-configmaps"><a class="header" href="#creating-cluster-templates-configmaps">Creating cluster templates ConfigMaps</a></h2>
<ol>
<li>Create a cluster template for Oracle Linux:</li>
</ol>
<pre><code class="language-shell">kubectl create cm oracletemplate --from-file=template=templates/cluster-template-oraclelinux.yaml
</code></pre>
<ol>
<li>Create a cluster template for Ubuntu:</li>
</ol>
<pre><code class="language-shell">kubectl create cm ubuntutemplate --from-file=template=templates/cluster-template.yaml
</code></pre>
<p>You can then reuse the <code>ConfigMap</code> to create your clusters. For example, to create a workload cluster using Oracle Linux, you can create it as follows:</p>
<pre><code class="language-shell">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;oracle-linux-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
CONTROL_PLANE_MACHINE_COUNT=1 \
KUBERNETES_VERSION=v1.20.10 \
NAMESPACE=default \
WORKER_MACHINE_COUNT=1 \
clusterctl generate cluster &lt;cluster-name&gt;\
--from-config-map oracletemplate | kubectl apply -f -
</code></pre>
<p>Likewise, to create a workload cluster using Ubuntu:</p>
<pre><code class="language-shell">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_IMAGE_ID=&lt;ubuntu-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
CONTROL_PLANE_MACHINE_COUNT=1 \
KUBERNETES_VERSION=v1.20.10 \
NAMESPACE=default \
WORKER_MACHINE_COUNT=1 \
clusterctl generate cluster &lt;cluster-name&gt;\
--from-config-map ubuntutemplate | kubectl apply -f -
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="externally-managed-cluster-infrastructure"><a class="header" href="#externally-managed-cluster-infrastructure">Externally managed Cluster infrastructure</a></h1>
<p>By default, Cluster API will create resources on Oracle Cloud Infrastructure (OCI) when instantiating a new workload cluster. However, it is possible to have Cluster API re-use an existing OCI infrastructure instead of creating a new one. The existing infrastructure could include:</p>
<ol>
<li>Virtual cloud networks (VCNs)</li>
<li>Network load balancers used as Kubernetes API Endpoint</li>
</ol>
<h2 id="example-spec-for-externally-managed-vcn-infrastructure"><a class="header" href="#example-spec-for-externally-managed-vcn-infrastructure">Example spec for externally managed VCN infrastructure</a></h2>
<p>CAPOCI can be used to create a cluster using existing VCN infrastructure. In this case, only the
API Server Load Balancer will be managed by CAPOCI.</p>
<p>Example spec is given below</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    skipNetworkManagement: true
    vcn:
      id: &lt;Insert VCN OCID Here&gt;
      networkSecurityGroup:
        list:
          - id: &lt;Insert Control Plane Endpoint NSG OCID Here&gt;
            role: control-plane-endpoint
            name: control-plane-endpoint
          - id: &lt;Insert Worker NSG OCID Here&gt;
            role: worker
            name: worker
          - id: &lt;Insert Control Plane NSG OCID Here&gt;
            role: control-plane
            name: control-plane
      subnets:
        - id: &lt;Insert Control Plane Endpoint Subnet OCID Here&gt;
          role: control-plane-endpoint
          name: control-plane-endpoint
        - id: &lt;Insert Worker Subnet OCID Here&gt;
          role: worker
          name: worker
        - id: &lt;Insert control Plane Subnet OCID Here&gt;
          role: control-plane
          name: control-plane
</code></pre>
<p>In the above spec, note that name has to be mentioned for Subnet/NSG. This is so that Kubernetes
can merge the list properly when there is an update.</p>
<h2 id="example-spec-for-externally-managed-vcn-subnet-internet-gateway-nat-gateway-service-gateway-and-routing-table-but-the-other-networking-components-are-still-managed-by-capoci-eg-network-security-groups"><a class="header" href="#example-spec-for-externally-managed-vcn-subnet-internet-gateway-nat-gateway-service-gateway-and-routing-table-but-the-other-networking-components-are-still-managed-by-capoci-eg-network-security-groups">Example spec for externally managed VCN, Subnet, Internet Gateway, Nat Gateway, Service Gateway and routing table, but the other networking components are still managed by CAPOCI (e.g. Network Security Groups)</a></h2>
<p>Example spec is given below</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      skip: true
      id: &lt;Insert VCN OCID Here&gt;  // REQUIRED
      networkSecurityGroup:
        skip: false 
      internetGateway:
        skip: true // REQUIRED
      natGateway:
        skip: true // REQUIRED
      serviceGateway:
        skip: true // REQUIRED
      routeTable:
        skip: true // REQUIRED
      subnets:
        - id: &lt;Insert control Plane Subnet OCID Here&gt;  // REQUIRED
          role: control-plane-endpoint
          name: control-plane-endpoint
          type: public
          skip: true
        - id: &lt;Insert control Plane Subnet OCID Here&gt;  // REQUIRED
          role: worker
          name: worker
          type: private
          skip: true
        - id: &lt;Insert control Plane Subnet OCID Here&gt;  // REQUIRED
          role: control-plane
          name: control-plane
          type: private
          skip: true
        - id: &lt;Insert control Plane Subnet OCID Here&gt;  // REQUIRED
          role: service-lb
          name: service-lb
          type: public
          skip: true
</code></pre>
<h2 id="example-ocicluster-spec-with-external-infrastructure"><a class="header" href="#example-ocicluster-spec-with-external-infrastructure">Example <code>OCICluster</code> Spec with external infrastructure</a></h2>
<p>CAPOCI supports <a href="https://github.com/kubernetes-sigs/cluster-api/blob/10d89ceca938e4d3d94a1d1c2b60515bcdf39829/docs/proposals/20210203-externally-managed-cluster-infrastructure.md">externally managed cluster infrastructure</a>.</p>
<p>If the <code>OCICluster</code> resource includes a <code>cluster.x-k8s.io/managed-by</code> annotation, then the <a href="https://cluster-api.sigs.k8s.io/developer/providers/cluster-infrastructure.html#normal-resource">controller will skip any reconciliation</a>.</p>
<p>This is useful for scenarios where a different persona is managing the cluster infrastructure out-of-band while still wanting to use CAPOCI for automated machine management.</p>
<p>The following <code>OCICluster</code> Spec includes the mandatory fields to be specified for externally managed infrastructure to work properly. In this example neither the VCN nor the network load balancer will be managed by CAPOCI.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: &quot;${CLUSTER_NAME}&quot;
  annotations:
    cluster.x-k8s.io/managed-by: &quot;external&quot;
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  controlPlaneEndpoint:
    host: &lt;Control Plane Endpoint Address should go here&gt;
    port: 6443
  networkSpec:
    apiServerLoadBalancer:
      loadBalancerId: &lt;OCID of Control Plane Endpoint LoadBalancer&gt;
    vcn:
      id: &lt;OCID of VCN&gt;
      networkSecurityGroup:
        list:
          - id: &lt;OCID of Control Plane NSG&gt;
            name: &lt;Name of Control Plane NSG&gt;
            role: control-plane
          - id: &lt;OCID of Worker NSG&gt;
            name: &lt;Name of Worker NSG&gt;
            role: worker
      subnets:
        - id: &lt;OCID of Control Plane Subnet&gt;
          role: control-plane
        - id: &lt;OCID of Worker Subnet&gt;
          role: worker
</code></pre>
<h2 id="status"><a class="header" href="#status">Status</a></h2>
<p>As per the Cluster API Provider specification, the <code>OCICluster Status</code> Object has to be updated with <code>ready</code> status
as well as the failure domain mapping. This has to be done after the <code>OCICluster</code> object has been created in the management cluster.
The following cURL request illustrates this:</p>
<p>Get a list of <a href="gs/../reference/glossary.html#ad">Availability Domains</a> of the region:</p>
<pre><code class="language-bash">oci iam availability-domain list
</code></pre>
<div id="admonition-info" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="gs/externally-managed-cluster-infrastructure.html#admonition-info"></a></p>
</div>
<div>
<p>Review the <a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/cliconcepts.htm">OCI CLI documentation</a> for more information regarding this tool.</p>
</div>
</div>
<p>For 1-AD regions, use the following cURL command to update the status object:</p>
<pre><code class="language-bash">curl -o  -s -X PATCH -H &quot;Accept: application/json, */*&quot; \
-H &quot;Content-Type: application/merge-patch+json&quot; \
--cacert ca.crt \
--cert client.crt \
--key client.key \
https://&lt;management-plane-api-endpoint&gt;/apis/infrastructure.cluster.x-k8s.io/v1beta2/namespaces/&lt;cluster-namespace&gt;/ociclusters/&lt;cluster-name&gt;/status \
--data '{&quot;status&quot;:{&quot;ready&quot;:true,&quot;failureDomains&quot;:{&quot;1&quot;:{&quot;attributes&quot;:{&quot;AvailabilityDomain&quot;:&quot;zkJl:AP-HYDERABAD-1-AD-1&quot;,&quot;FaultDomain&quot;:&quot;FAULT-DOMAIN-1&quot;},&quot;controlPlane&quot;:true},&quot;2&quot;:{&quot;attributes&quot;:{&quot;AvailabilityDomain&quot;:&quot;zkJl:AP-HYDERABAD-1-AD-1&quot;,&quot;FaultDomain&quot;:&quot;FAULT-DOMAIN-2&quot;},&quot;controlPlane&quot;:true},&quot;3&quot;:{&quot;attributes&quot;:{&quot;AvailabilityDomain&quot;:&quot;zkJl:AP-HYDERABAD-1-AD-1&quot;,&quot;FaultDomain&quot;:&quot;FAULT-DOMAIN-3&quot;}}}}}'
</code></pre>
<p>For 3-AD regions, use the following cURL command to update the status object:</p>
<pre><code class="language-bash">curl -o  -s -X PATCH -H &quot;Accept: application/json, */*&quot; \
-H &quot;Content-Type: application/merge-patch+json&quot; \
--cacert ca.crt \
--cert client.crt \
--key client.key \
https://&lt;management-plane-api-endpoint&gt;/apis/infrastructure.cluster.x-k8s.io/v1beta2/namespaces/&lt;cluster-namespace&gt;/ociclusters/&lt;cluster-name&gt;/status \
--data '{&quot;status&quot;:{&quot;ready&quot;:true,&quot;failureDomains&quot;:{&quot;1&quot;:{&quot;attributes&quot;:{&quot;AvailabilityDomain&quot;:&quot;zkJl:US-ASHBURN-1-AD-1&quot;},&quot;controlPlane&quot;:true},&quot;2&quot;:{&quot;attributes&quot;:{&quot;AvailabilityDomain&quot;:&quot;zkJl:US-ASHBURN-1-AD-2&quot;},&quot;controlPlane&quot;:true},&quot;3&quot;:{&quot;attributes&quot;:{&quot;AvailabilityDomain&quot;:&quot;zkJl:US-ASHBURN-1-AD-3&quot;}}}}}'
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-oracle-cloud-infrastructure-cloud-controller-manager"><a class="header" href="#install-oracle-cloud-infrastructure-cloud-controller-manager">Install Oracle Cloud Infrastructure Cloud Controller Manager</a></h1>
<p><a href="https://github.com/oracle/oci-cloud-controller-manager">Oracle Cloud Infrastructure (OCI) Cloud Controller Manager</a> is OCI's implementation of the Kubernetes <a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/">control plane component</a> that links your Kubernetes cluster to OCI.</p>
<h2 id="configure-authentication-via-instance-principal-recommended"><a class="header" href="#configure-authentication-via-instance-principal-recommended">Configure authentication via Instance Principal (Recommended)</a></h2>
<p>Oracle recommends using <a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/callingservicesfrominstances.htm">Instance principals</a> to be used by CCM for authentication. Please ensure the
following policies in the dynamic group for CCM to be able to talk to various OCI Services.</p>
<pre><code>allow dynamic-group [your dynamic group name] to read instance-family in compartment [your compartment name]
allow dynamic-group [your dynamic group name] to use virtual-network-family in compartment [your compartment name]
allow dynamic-group [your dynamic group name] to manage load-balancers in compartment [your compartment name]
</code></pre>
<ol>
<li>
<p>Download the example configuration file:</p>
<pre><code class="language-shell">curl -L https://raw.githubusercontent.com/oracle/oci-cloud-controller-manager/master/manifests/provider-config-instance-principals-example.yaml -o cloud-provider-example.yaml
</code></pre>
</li>
<li>
<p>Update values in the configuration file as necessary.</p>
<p>As an example using the provided <a href="https://github.com/oracle/cluster-api-provider-oci/blob/main/templates/cluster-template.yaml"><code>cluster-template.yaml</code></a>
you would modify the <code>cloud-provider-example.yaml</code> and make sure to set  <code>compartment</code> and <code>vcn</code> with the correct OCIDs.
Then set <code>subnet1</code> to the OCID of your <code>service-lb</code> subnet and remove <code>subnet2</code>. You would then set
<code>securityListManagementMode</code> to <code>&quot;None&quot;</code>.</p>
</li>
<li>
<p>Create a secret:</p>
<pre><code class="language-shell">kubectl  create secret generic oci-cloud-controller-manager \
  -n kube-system                                           \
  --from-file=cloud-provider.yaml=cloud-provider-example.yaml
</code></pre>
</li>
</ol>
<h2 id="install-ccm"><a class="header" href="#install-ccm">Install CCM</a></h2>
<ol>
<li>
<p>Navigate to the <a href="https://github.com/oracle/oci-cloud-controller-manager/releases">release page</a> of CCM and export the version that you want to install. Typically,
the latest version can be installed.</p>
<pre><code class="language-shell">export CCM_RELEASE_VERSION=&lt;update-version-here&gt;
</code></pre>
</li>
<li>
<p>Download the deployment manifests:</p>
<pre><code class="language-shell">curl -L &quot;https://github.com/oracle/oci-cloud-controller-manager/releases/download/${CCM_RELEASE_VERSION}/oci-cloud-controller-manager.yaml&quot; -o oci-cloud-controller-manager.yaml

curl -L &quot;https://github.com/oracle/oci-cloud-controller-manager/releases/download/${CCM_RELEASE_VERSION}/oci-cloud-controller-manager-rbac.yaml&quot; -o oci-cloud-controller-manager-rbac.yaml
</code></pre>
</li>
<li>
<p>Deploy the CCM:</p>
<pre><code class="language-shell">kubectl apply -f oci-cloud-controller-manager.yaml
</code></pre>
</li>
<li>
<p>Deploy the RBAC rules:</p>
<pre><code class="language-shell">kubectl apply -f oci-cloud-controller-manager-rbac.yaml
</code></pre>
</li>
<li>
<p>Check the CCM logs to verify OCI CCM is running correctly:</p>
<pre><code class="language-shell">kubectl -n kube-system get po | grep oci
oci-cloud-controller-manager-ds-k2txq   1/1       Running   0          19s

kubectl -n kube-system logs oci-cloud-controller-manager-ds-k2txq
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="install-csi"><a class="header" href="#install-csi">Install CSI</a></h1>
<p>On Oracle Cloud Infrastructure (OCI), there are two types of storage services available to store persistent data:</p>
<ul>
<li>OCI Block Volume Service</li>
<li>OCI File Storage Service</li>
</ul>
<p>A persistent volume claim (PVC) is a request for storage, which is met by binding the PVC to a persistent volume (PV). A PVC provides an abstraction layer to the underlying storage. CSI drivers for both the Block Volume Service and File Storage Service have been implemented.</p>
<h2 id="configure-authentication-via-instance-principal"><a class="header" href="#configure-authentication-via-instance-principal">Configure authentication via Instance Principal</a></h2>
<p>Oracle recommends using <a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/callingservicesfrominstances.htm">Instance principals</a> to be used by CSI for authentication. Please ensure the 
following policies in the dynamic group for CSI to be able to talk to various OCI Services.</p>
<pre><code>allow dynamic-group [your dynamic group name] to use instance-family in compartment [your compartment name]
allow dynamic-group [your dynamic group name] to use virtual-network-family in compartment [your compartment name]
allow dynamic-group [your dynamic group name] to manage volume-family in compartment [your compartment name]
</code></pre>
<ol>
<li>
<p>Download the example configuration file:</p>
<pre><code class="language-shell">curl -L https://raw.githubusercontent.com/oracle/oci-cloud-controller-manager/master/manifests/provider-config-instance-principals-example.yaml -o cloud-provider-example.yaml
</code></pre>
</li>
<li>
<p>Update values in the configuration file as necessary.</p>
</li>
<li>
<p>Create a secret:</p>
<pre><code class="language-shell">kubectl  create secret generic oci-volume-provisioner \
  -n kube-system                                           \
  --from-file=config.yaml=cloud-provider-example.yaml
</code></pre>
</li>
</ol>
<h3 id="install-csi-drivers"><a class="header" href="#install-csi-drivers">Install CSI Drivers</a></h3>
<ol>
<li>
<p>Navigate to the <a href="https://github.com/oracle/oci-cloud-controller-manager/releases">release page</a> of CCM and export the version that you want to install. Typically, 
the latest version can be installed.</p>
<pre><code class="language-shell">export CCM_RELEASE_VERSION=&lt;update-version-here&gt;
</code></pre>
</li>
<li>
<p>Download the deployment manifests:</p>
<pre><code class="language-shell">curl -L &quot;https://github.com/oracle/oci-cloud-controller-manager/releases/download/${CCM_RELEASE_VERSION}/oci-csi-node-rbac.yaml&quot; -o oci-csi-node-rbac.yaml

curl -L &quot;https://github.com/oracle/oci-cloud-controller-manager/releases/download/${CCM_RELEASE_VERSION}/oci-csi-controller-driver.yaml&quot; -o oci-csi-controller-driver.yaml

curl -L h&quot;ttps://github.com/oracle/oci-cloud-controller-manager/releases/download/${CCM_RELEASE_VERSION}/oci-csi-node-driver.yaml&quot; -o
oci-csi-node-driver.yaml

curl -L https://raw.githubusercontent.com/oracle/oci-cloud-controller-manager/master/manifests/container-storage-interface/storage-class.yaml -o storage-class.yaml
</code></pre>
</li>
<li>
<p>Create the RBAC rules:</p>
<pre><code class="language-shell">kubectl apply -f oci-csi-node-rbac.yaml
</code></pre>
</li>
<li>
<p>Deploy the csi-controller-driver. It is provided as a deployment and it has three containers:</p>
<ul>
<li><code>csi-provisioner external-provisioner</code></li>
<li><code>csi-attacher external-attacher</code></li>
<li><code>oci-csi-controller-driver</code></li>
</ul>
</li>
</ol>
<pre><code class="language-shell"> kubectl apply -f oci-csi-controller-driver.yaml
</code></pre>
<ol>
<li>
<p>Deploy the <code>node-driver</code>. It is provided as a daemon set and it has two containers:</p>
<ul>
<li><code>node-driver-registrar</code></li>
<li><code>oci-csi-node-driver</code></li>
</ul>
<pre><code class="language-shell">kubectl apply -f oci-csi-node-driver.yaml
</code></pre>
</li>
<li>
<p>Create the CSI storage class for the Block Volume Service:</p>
<pre><code class="language-shell">kubectl apply -f storage-class.yaml
</code></pre>
</li>
<li>
<p>Verify the <code>oci-csi-controller-driver</code> and <code>oci-csi-node-controller</code> are running in your cluster:</p>
<pre><code class="language-shell">kubectl -n kube-system get po | grep csi-oci-controller
kubectl -n kube-system get po | grep csi-oci-node
</code></pre>
</li>
</ol>
<h3 id="provision-pvcs"><a class="header" href="#provision-pvcs">Provision PVCs</a></h3>
<p>Follow the guides below to create PVCs based on the service you require:</p>
<ul>
<li>
<p><a href="gs/pvc-pvc-bv.html">Provision a PVC on the Block Volume Service</a></p>
</li>
<li>
<p><a href="gs/pvc-fss.html">Provision a PVC on the File Storage Service</a></p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provision-a-pvc-on-the-block-volume-service"><a class="header" href="#provision-a-pvc-on-the-block-volume-service">Provision a PVC on the Block Volume Service</a></h1>
<p>The Oracle Cloud Infrastructure Block Volume service (the Block Volume service) provides persistent, durable, and high-performance block storage for your data.</p>
<h2 id="create-a-block-volume-dynamically-using-a-new-pvc"><a class="header" href="#create-a-block-volume-dynamically-using-a-new-pvc">Create a block volume dynamically using a new PVC</a></h2>
<p>If the cluster administrator has not created any suitable PVs that match the PVC request, you can dynamically provision a block volume using the CSI plugin specified by the oci-bv storage class's definition (provisioner: blockvolume.csi.oraclecloud.com).</p>
<ol>
<li>
<p>Define a PVC in a yaml file (<code>csi-bvs-pvc.yaml</code>)as below:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: mynginxclaim
spec:
 storageClassName: &quot;oci-bv&quot;
 accessModes:
   - ReadWriteOnce
 resources:
   requests:
     storage: 50Gi
</code></pre>
</li>
<li>
<p>Apply the manifest to create the PVC</p>
<pre><code class="language-shell">kubectl create -f  csi-bvs-pvc.yaml
</code></pre>
</li>
<li>
<p>Verify that the PVC has been created:</p>
<pre><code class="language-shell">kubectl get pvc
</code></pre>
</li>
<li>
<p>The output from the above command shows the current status of the PVC:</p>
<pre><code class="language-shell">NAME               STATUS   VOLUME   CAPACITY   ACCESSMODES   STORAGECLASS   AGE
mynginxclaim       Pending                                    oci-bv         4m
</code></pre>
<p>The PVC has a status of Pending because the <code>oci-bv</code> storage class's definition includes <code>volumeBindingMode: WaitForFirstConsumer</code>.</p>
</li>
<li>
<p>You can use this PVC when creating other objects, such as pods. For example, you could create a new pod from the following pod definition, which instructs the system to use the <code>mynginxclaim</code> PVC as the NGINX volume, which is mounted by the pod at <code>/data</code>.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Pod
metadata:
 name: nginx
spec:
 containers:
   - name: nginx
     image: nginx:latest
     ports:
       - name: http
         containerPort: 80
     volumeMounts:
       - name: data
         mountPath: /usr/share/nginx/html
 volumes:
   - name: data
     persistentVolumeClaim:
     claimName: mynginxclaim
</code></pre>
</li>
<li>
<p>After creating the new pod, verify that the PVC has been bound to a new persistent volume (PV):</p>
<pre><code class="language-shell">kubectl get pvc
</code></pre>
<p>The output from the above command confirms that the PVC has been bound:</p>
<pre><code class="language-shell">NAME               STATUS    VOLUME                               CAPACITY   ACCESSMODES   STORAGECLASS   AGE
mynginxclaim       Bound     ocid1.volume.oc1.iad.&lt;unique_ID&gt;     50Gi       RWO           oci-bv         4m
</code></pre>
</li>
<li>
<p>Verify that the pod is using the new PVC:</p>
<pre><code class="language-shell">kubectl describe pod nginx
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="provision-a-pvc-on-the-file-storage-service"><a class="header" href="#provision-a-pvc-on-the-file-storage-service">Provision a PVC on the File Storage Service</a></h1>
<p>The Oracle Cloud Infrastructure File Storage service provides a durable, scalable, distributed, enterprise-grade network file system.</p>
<h2 id="provision-pvcs-on-the-file-storage-service-fss"><a class="header" href="#provision-pvcs-on-the-file-storage-service-fss">Provision PVCs on the File Storage Service (FSS)</a></h2>
<p>Provisioning PVCs on FSS consists of 3 steps:</p>
<ol>
<li>Create an FSS instance and a mount target</li>
<li>Define and create a PV backed by FSS</li>
<li>Define and create a PVC provisioned by the PV</li>
</ol>
<h3 id="configure-the-vcn-and-create-an-fss"><a class="header" href="#configure-the-vcn-and-create-an-fss">Configure the VCN and create an FSS</a></h3>
<ol>
<li>Choose your <a href="https://docs.oracle.com/en-us/iaas/Content/File/Tasks/securitylistsfilestorage.htm#File_Storage_Security_Rule_Scenarios">deployment scenario</a>:
<ol>
<li>Mount target and worker nodes in the same subnet</li>
<li>Mount target and worker nodes in different subnets</li>
<li>Mount target and instance use in-transit encryption</li>
</ol>
</li>
<li>Implement <a href="https://docs.oracle.com/en-us/iaas/Content/File/Tasks/securitylistsfilestorage.htm#Setting2">security rules</a> based on your deployment scenario</li>
<li><a href="https://docs.oracle.com/en-us/iaas/Content/File/Tasks/creatingfilesystems.htm#createfs">Create a FSS instance</a></li>
</ol>
<h3 id="define-and-create-a-pv-backed-by-fss"><a class="header" href="#define-and-create-a-pv-backed-by-fss">Define and create a PV backed by FSS</a></h3>
<ol>
<li>
<p>Create a yaml file (<code>fss-pv.yaml</code>) to define the PV:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolume
metadata:
  name: fss-pv
spec:
 capacity:
   storage: 50Gi
 volumeMode: Filesystem
 accessModes:
   - ReadWriteMany
 persistentVolumeReclaimPolicy: Retain
 csi:
   driver: fss.csi.oraclecloud.com
   volumeHandle: ocid1.filesystem.oc1.iad.aaaa______j2xw:10.0.0.6:/FileSystem1
</code></pre>
</li>
<li>
<p>In the yaml file, set:</p>
<ol>
<li><code>driver</code> value to: <code>fss.csi.oraclecloud.com</code></li>
<li><code>volumeHandle</code> value to: <code>&lt;FileSystemOCID&gt;:&lt;MountTargetIP&gt;:&lt;path&gt;</code></li>
</ol>
<p>The <code>&lt;FileSystemOCID&gt;</code> is the OCID of the file system defined in the File Storage service, the <code>&lt;MountTargetIP&gt;</code> is the IP address assigned to the mount target and the <code>&lt;path&gt;</code> is the mount path to the file system relative to the mount target IP address, starting with a slash.</p>
</li>
<li>
<p>Create the PV from the manifest file:</p>
<pre><code class="language-shell">kubectl create -f fss-pv.yaml
</code></pre>
</li>
</ol>
<h3 id="define-and-create-a-pvc-provisioned-by-the-pv"><a class="header" href="#define-and-create-a-pvc-provisioned-by-the-pv">Define and create a PVC provisioned by the PV</a></h3>
<ol>
<li>
<p>Create a manifest file (<code>fss-pvc.yaml</code>) to define the PVC:</p>
<pre><code class="language-yaml">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: fss-pvc
spec:
  accessModes:
    - ReadWriteMany
  storageClassName: &quot;&quot;
  resources:
    requests:
      storage: 50Gi
  volumeName: fss-pv
</code></pre>
</li>
<li>
<p>In the YAML file, set:</p>
<ol>
<li><code>storageClassName</code> to <code>&quot;&quot;</code></li>
<li><code>volumeName</code> to the name of the PV created earlier</li>
</ol>
</li>
<li>
<p>Create the PVC from the manifest file:</p>
<pre><code class="language-shell">kubectl create -f fss-pvc.yaml
</code></pre>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="customizing-worker-nodes"><a class="header" href="#customizing-worker-nodes">Customizing worker nodes</a></h1>
<h2 id="configure-user-managed-boot-volume-encryption"><a class="header" href="#configure-user-managed-boot-volume-encryption">Configure user managed boot volume encryption</a></h2>
<p>Use the following configuration in <code>OCIMachineTemplate</code> to use a <a href="https://docs.oracle.com/en-us/iaas/Content/KeyManagement/Tasks/assigningkeys.htm">customer
managed boot volume encryption key</a>.</p>
<pre><code class="language-yaml">kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  template:
    spec:
      instanceSourceViaImageConfig:
        kmsKeyId: &lt;kms-key-id&gt;
</code></pre>
<h2 id="configure-shielded-instances"><a class="header" href="#configure-shielded-instances">Configure shielded instances</a></h2>
<p>Use the following configuration in <code>OCIMachineTemplate</code> to create <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/shielded-instances.htm">shielded instances</a>.
Below example is for an AMD based VM. Please read the <a href="https://github.com/oracle/cluster-api-provider-oci/blob/main/api/v1beta1/types.go">CAPOCI github page</a> PlatformConfig struct
for an enumeration of all the possible configurations.</p>
<pre><code class="language-yaml">kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  template:
    spec:
      platformConfig:
        platformConfigType: &quot;AMD_VM&quot;
        amdVmPlatformConfig:
          isSecureBootEnabled: true
          isTrustedPlatformModuleEnabled: true
          isMeasuredBootEnabled: true
</code></pre>
<h2 id="configure-confidential-instances"><a class="header" href="#configure-confidential-instances">Configure confidential instances</a></h2>
<p>Use the following configuration in <code>OCIMachineTemplate</code> to create <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/confidential_compute.htm">confidential instances</a>.
Below example is for an AMD based VM. Please read the <a href="https://github.com/oracle/cluster-api-provider-oci/blob/main/api/v1beta1/types.go">CAPOCI github page</a> PlatformConfig struct
for an enumeration of all the possible configurations.</p>
<pre><code class="language-yaml">kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  template:
    spec:
      platformConfig:
        platformConfigType: &quot;AMD_VM&quot;
        amdVmPlatformConfig:
          isMemoryEncryptionEnabled: true
</code></pre>
<h2 id="configure-preemptible-instances"><a class="header" href="#configure-preemptible-instances">Configure preemptible instances</a></h2>
<p>Use the following configuration in <code>OCIMachineTemplate</code> to create <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/Concepts/preemptible.htm#howitworks__using">preemtible instances</a>.</p>
<pre><code class="language-yaml">kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  template:
    spec:
      preemptibleInstanceConfig:
        terminatePreemptionAction:
          preserveBootVolume: false
</code></pre>
<h2 id="configure-capacity-reservation"><a class="header" href="#configure-capacity-reservation">Configure capacity reservation</a></h2>
<p>Use the following configuration in <code>OCIMachineTemplate</code> to use <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/reserve-capacity.htm">capacity reservations</a>.</p>
<pre><code class="language-yaml">kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  template:
    spec:
      capacityReservationId: &lt;capacity-reservation-id&gt;
</code></pre>
<h2 id="configure-oracle-cloud-agent-plugins"><a class="header" href="#configure-oracle-cloud-agent-plugins">Configure Oracle Cloud Agent plugins</a></h2>
<p>Use the following configuration in <code>OCIMachineTemplate</code> to configure <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/Tasks/manage-plugins.htm">Oracle Cloud Agent plugins</a>.
The example below enables Bastion plugin.</p>
<pre><code class="language-yaml">kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  template:
    spec:
      agentConfig:
        pluginsConfigs:
          - name: &quot;Bastion&quot;
            desiredState: &quot;ENABLED&quot;
</code></pre>
<h2 id="configure-burstable-instances"><a class="header" href="#configure-burstable-instances">Configure Burstable Instances</a></h2>
<p>Use the following configuration in <code>OCIMachineTemplate</code> to configure <a href="https://docs.oracle.com/en-us/iaas/Content/Compute/References/burstable-instances.htm">Burstable Instance</a>.
The following values are supported for <code>baselineOcpuUtilization</code>.</p>
<ul>
<li>BASELINE_1_8 - baseline usage is 1/8 of an OCPU.</li>
<li>BASELINE_1_2 - baseline usage is 1/2 of an OCPU.</li>
<li>BASELINE_1_1 - baseline usage is an entire OCPU. This represents a non-burstable instance.</li>
</ul>
<pre><code class="language-yaml">kind: OCIMachineTemplate
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  template:
    spec:
      shapeConfig:
        baselineOcpuUtilization: &quot;BASELINE_1_8&quot;
        ocpus: &quot;1&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multi-tenancy"><a class="header" href="#multi-tenancy">Multi-tenancy</a></h1>
<p>CAPOCI supports multi-tenancy wherein different OCI user principals can be used to reconcile 
different OCI clusters. This is achieved by associating a cluster with a Cluster Identity and
associating the identity with a user principal. Currently only OCI user principal is supported
for Cluster Identity.</p>
<h1 id="steps"><a class="header" href="#steps">Steps</a></h1>
<h2 id="step-1---create-a-secret-with-user-principal-in-the-management-cluster"><a class="header" href="#step-1---create-a-secret-with-user-principal-in-the-management-cluster">Step 1 - Create a secret with user principal in the management cluster</a></h2>
<p>Please read the <a href="https://docs.oracle.com/en-us/iaas/Content/API/Concepts/apisigningkey.htm#Required_Keys_and_OCIDs">doc</a> to know more about the parameters below.</p>
<pre><code class="language-yaml">apiVersion: v1
kind: Secret
metadata:
  name: user-credentials
  namespace: default
type: Opaque
data:
  tenancy: &lt;base-64-encoded value of tenancy OCID&gt;
  user: &lt;base-64-encoded value of user OCID&gt;
  key: &lt;base-64-encoded value of user Key&gt;
  fingerprint: &lt;base-64-encoded value of fingerprint&gt;
  passphrase: &lt;base-64-encoded value of passphrase. This is optional&gt;
  region: &lt;base-64-encoded value of region. This is optional&gt;
</code></pre>
<h2 id="step-2---edit-the-cluster-template-to-add-a-cluster-identity-section-and-point-the-ocicluster-to-the-cluster-identity"><a class="header" href="#step-2---edit-the-cluster-template-to-add-a-cluster-identity-section-and-point-the-ocicluster-to-the-cluster-identity">Step 2 - Edit the cluster template to add a Cluster Identity section and point the OCICluster to the Cluster Identity</a></h2>
<p>The Cluster Identity should have a reference to the secret created above.</p>
<pre><code class="language-yaml">---
kind: OCIClusterIdentity
metadata:
  name: cluster-identity
  namespace: default
spec:
  type: UserPrincipal
  principalSecret:
    name: user-credentials
    namespace: default
  allowedNamespaces: {}
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: &quot;${CLUSTER_NAME}&quot;
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  identityRef:
    apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
    kind: OCIClusterIdentity
    name: cluster-identity
    namespace: default
</code></pre>
<h1 id="allowednamespaces"><a class="header" href="#allowednamespaces">allowedNamespaces</a></h1>
<p><code>allowedNamespaces</code> can be used to control which namespaces the <code>OCIClusters</code> are allowed to use the identity from. 
Namespaces can be selected either using an array of namespaces or with label selector.
An empty <code>allowedNamespaces</code> object indicates that <code>OCIClusters</code> can use this identity from any namespace.
If this object is <code>nil</code>, no namespaces will be allowed, which is the default behavior of the field if not specified.</p>
<blockquote>
<p>Note: NamespaceList will take precedence over Selector if both are set.</p>
</blockquote>
<h2 id="cluster-identity-using-instance-principals"><a class="header" href="#cluster-identity-using-instance-principals">Cluster Identity using Instance Principals</a></h2>
<p>Cluster Identity also supports <a href="https://docs.oracle.com/en-us/iaas/Content/Identity/Tasks/callingservicesfrominstances.htm">Instance Principals</a>. The example <code>OCIClusterIdentity</code>
spec shown below uses Instance Principals.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIClusterIdentity
metadata:
  name: cluster-identity
  namespace: default
spec:
  type: InstancePrincipal
  allowedNamespaces: {}
</code></pre>
<h2 id="cluster-identity-using-workload-identity"><a class="header" href="#cluster-identity-using-workload-identity">Cluster Identity using Workload Identity</a></h2>
<p>Cluster Identity supports <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contenggrantingworkloadaccesstoresources.htm">Workload</a> access to OCI resources also knows as Workload Identity. The example 
<code>OCIClusterIdentity</code> spec shown below uses Workload Identity.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIClusterIdentity
metadata:
  name: cluster-identity
  namespace: default
spec:
  type: Workload
  allowedNamespaces: {}
</code></pre>
<p>CAPOCI, by default create a Service Account <code>capoci-controller-manager</code> in namespace <code>cluster-api-provider-oci-system</code>.
Workload identity needs to have policies required to create OKE or Self managed clusters. For example, the following
policies will provide Workload identity with permissions to create OKE cluster.</p>
<ul>
<li><code>Allow any-user to manage virtual-network-family in compartment &lt;compartment&gt; where all { request.principal.type = 'workload', request.principal.namespace = 'cluster-api-provider-oci-system', request.principal.service_account = 'capoci-controller-manager'}</code></li>
<li><code>Allow any-user to manage cluster-family in compartment &lt;compartment&gt; where all { request.principal.type = 'workload', request.principal.namespace = 'cluster-api-provider-oci-system', request.principal.service_account = 'capoci-controller-manager'}</code></li>
<li><code>Allow any-user to manage volume-family in compartment &lt;compartment&gt; where all { request.principal.type = 'workload', request.principal.namespace = 'cluster-api-provider-oci-system', request.principal.service_account = 'capoci-controller-manager'}</code></li>
<li><code>Allow any-user to manage instance-family in compartment &lt;compartment&gt; where all { request.principal.type = 'workload', request.principal.namespace = 'cluster-api-provider-oci-system', request.principal.service_account = 'capoci-controller-manager'}</code></li>
<li><code>Allow any-user to inspect compartments in compartment &lt;compartment&gt; where all { request.principal.type = 'workload', request.principal.namespace = 'cluster-api-provider-oci-system', request.principal.service_account = 'capoci-controller-manager'}</code></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="advanced-options"><a class="header" href="#advanced-options">Advanced Options</a></h1>
<h2 id="disable-oci-client-initialization-on-startup"><a class="header" href="#disable-oci-client-initialization-on-startup">Disable OCI Client initialization on startup</a></h2>
<p>CAPOCI supports setting OCI principals at <a href="gs/./multi-tenancy.html">cluster level</a>, hence CAPOCI can be
installed without providing OCI user credentials. The following environment variable need to be exported
to install CAPOCI without providing any OCI credentials.</p>
<pre><code class="language-shell">export INIT_OCI_CLIENTS_ON_STARTUP=false
</code></pre>
<p>If the above setting is used, and <a href="gs/./multi-tenancy.html">Cluster Identity</a> is not used, the OCICluster will
go into error state, and the following error will show up in the CAPOCI pod logs.</p>
<p><code>OCI authentication credentials could not be retrieved from pod or cluster level,please install Cluster API Provider for OCI with OCI authentication credentials or set Cluster Identity in the OCICluster</code></p>
<h2 id="setup-heterogeneous-cluster"><a class="header" href="#setup-heterogeneous-cluster">Setup heterogeneous cluster</a></h2>
<blockquote>
<p>This section assumes you have <a href="gs/./create-windows-workload-cluster.html">setup a Windows workload cluster</a>.</p>
</blockquote>
<p>To add Linux nodes to the existing Windows workload cluster use the following YAML as a guide to provision 
just the new Linux machines.</p>
<p>Create a file and call it <code>cluster-template-windows-calico-heterogeneous.yaml</code>. Then add the following:</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
kind: OCIMachineTemplate
metadata:
  name: &quot;${CLUSTER_NAME}-md-0&quot;
spec:
  template:
    spec:
      imageId: &quot;${OCI_IMAGE_ID}&quot;
      compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
      shape: &quot;${OCI_NODE_MACHINE_TYPE=VM.Standard.E4.Flex}&quot;
      shapeConfig:
        ocpus: &quot;${OCI_NODE_MACHINE_TYPE_OCPUS=1}&quot;
      metadata:
        ssh_authorized_keys: &quot;${OCI_SSH_KEY}&quot;
      isPvEncryptionInTransitEnabled: ${OCI_NODE_PV_TRANSIT_ENCRYPTION=true}
---
apiVersion: bootstrap.cluster.x-k8s.io/v1alpha4
kind: KubeadmConfigTemplate
metadata:
  name: &quot;${CLUSTER_NAME}-md-0&quot;
spec:
  template:
    spec:
      joinConfiguration:
        nodeRegistration:
          kubeletExtraArgs:
            cloud-provider: external
            provider-id: oci://{{ ds[&quot;id&quot;] }}
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: &quot;${CLUSTER_NAME}-md-0&quot;
spec:
  clusterName: &quot;${CLUSTER_NAME}&quot;
  replicas: ${NODE_MACHINE_COUNT}
  selector:
    matchLabels:
  template:
    spec:
      clusterName: &quot;${CLUSTER_NAME}&quot;
      version: &quot;${KUBERNETES_VERSION}&quot;
      bootstrap:
        configRef:
          name: &quot;${CLUSTER_NAME}-md-0&quot;
          apiVersion: bootstrap.cluster.x-k8s.io/v1beta1
          kind: KubeadmConfigTemplate
      infrastructureRef:
        name: &quot;${CLUSTER_NAME}-md-0&quot;
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1
        kind: OCIMachineTemplate
</code></pre>
<p>Then apply the template</p>
<pre><code class="language-bash">OCI_IMAGE_ID=&lt;your new linux image OCID&gt; \
OCI_NODE_IMAGE_ID=&lt;your new linux image OCID&gt; \
OCI_COMPARTMENT_ID=&lt;your compartment&gt; \
NODE_MACHINE_COUNT=2 \
OCI_NODE_MACHINE_TYPE=&lt;shape&gt; \
OCI_NODE_MACHINE_TYPE_OCPUS=4 \
OCI_SSH_KEY=&quot;&lt;your public ssh key&gt;&quot; \
clusterctl generate cluster &lt;cluster-name&gt; --kubernetes-version &lt;kubernetes-version&gt; \
--target-namespace default \
--from cluster-template-windows-calico-heterogeneous.yaml | kubectl apply -f -
</code></pre>
<p>After a few minutes the instances will come up and the CNI will be installed.</p>
<h3 id="node-constraints"><a class="header" href="#node-constraints">Node constraints</a></h3>
<p>All future deployments make sure to setup node constraints using something like <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector"><code>nodeselctor</code></a>. Example:</p>
<div class="table-wrapper"><table><thead><tr><th>Windows</th><th>Linux</th></tr></thead><tbody>
<tr><td><code>nodeSelector: kubernetes.io/os: windows</code></td><td><code>nodeSelector:kubernetes.io/os: linux</code></td></tr>
</tbody></table>
</div><br/>
<details>
  <summary>nodeSelector examples - click to expand</summary>
<p>Linux nginx deployment example:</p>
<pre><code class="language-bash">apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-nginx-linux
spec:
  selector:
    matchLabels:
      run: my-nginx-linux
  replicas: 2
  template:
    metadata:
      labels:
        run: my-nginx-linux
    spec:
      nodeSelector:
        kubernetes.io/os: linux
      containers:
      - args:
        - /bin/sh
        - -c
        - sleep 3600
        name: nginx
        image: nginx:latest
</code></pre>
<p>For a Windows deployment example see the <a href="https://kubernetes.io/docs/concepts/windows/user-guide/#getting-started-deploying-a-windows-workload">Kubernetes Getting Started: Deploying a Windows workload</a> documentation</p>
</details>
<p>Without doing this it is possible that the Kubernetes scheduler will try to deploy your Windows pods onto a Linux worker, or vice versa.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="networking-guide"><a class="header" href="#networking-guide">Networking Guide</a></h1>
<p>This section contains information about the networking aspects of Cluster API Provider OCI.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="default-network-infrastructure"><a class="header" href="#default-network-infrastructure">Default Network Infrastructure</a></h1>
<p>The diagram below depicts the networking architecture for a public workload cluster created in a region such as <strong>US West (Phoenix)</strong>.
<img src="networking/../images/clusteroci.svg" alt="Networking Architecture - Workload Cluster" /></p>
<p>Each workload cluster requires an <a href="networking/../reference/glossary.html#vcn">OCI Virtual Cloud Network (VCN)</a> which houses all the resources created for the workload cluster. The default VCN has the following resources:</p>
<ul>
<li>
<p>Gateways:</p>
<ol>
<li>An <a href="networking/../reference/glossary.html#internet-gateway">Internet Gateway</a></li>
<li>A <a href="networking/../reference/glossary.html#nat-gateway">NAT Gateway</a></li>
<li>A <a href="networking/../reference/glossary.html#service-gateway">Service Gateway</a></li>
</ol>
</li>
<li>
<p>Route Tables:</p>
<ol>
<li>A route table for <a href="https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/overview.htm#Public">public subnets</a> which will route stateful traffic to and from the Internet Gateway</li>
<li>A route table for <a href="https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/overview.htm#Public">private subnets</a> which will route stateful traffic to and from the NAT and Service Gateways</li>
</ol>
</li>
<li>
<p>Subnets:</p>
<ol>
<li>A public Control plane endpoint subnet which houses an OCI Load Balancer. The load balancer acts as a reverse proxy for the Kubernetes API Server.</li>
<li>A private Control plane subnet which houses the Control plane nodes. The Control plane nodes run the Kubernetes Control plane components such as the API Server and the Control plane pods.</li>
<li>A public subnet which houses the service load balancers.</li>
<li>A private subnet which houses the worker nodes.</li>
</ol>
</li>
<li>
<p>Network Security Groups (NSG):</p>
<ol>
<li>An NSG for the Control plane endpoint (Control plane Endpoint NSG)</li>
<li>An NSG for the Kubernetes Control plane nodes (Control plane NSG)</li>
<li>An NSG for the service load balancers (Worker NSG)</li>
<li>An NSG for the Kubernetes worker nodes (Service Load Balancers NSG)</li>
</ol>
</li>
</ul>
<p>The sections below list the security rules required for the NSGs in each of the following <a href="networking/../reference/glossary.html#cni">CNI</a> providers:</p>
<ul>
<li><a href="networking/calico.html">Using Calico</a></li>
<li><a href="networking/antrea.html">Using Antrea</a></li>
</ul>
<p>Currently, the following providers have been tested and verified to work:</p>
<div class="table-wrapper"><table><thead><tr><th>CNI</th><th>CNI Version</th><th>Kubernetes Version</th><th>CAPOCI Version</th></tr></thead><tbody>
<tr><td><a href="networking/calico.html">Calico</a></td><td>3.21</td><td>1.20.10</td><td>0.1</td></tr>
<tr><td><a href="networking/antrea.html">Antrea</a></td><td></td><td>1.20.10</td><td>0.1</td></tr>
</tbody></table>
</div>
<p>If you have tested an alternative CNI provider and verified it to work, please send us a PR to add it to the list. Your PR for your tested CNI provider should include the following:</p>
<ul>
<li>CNI provider version tested</li>
<li>Documentation of NSG rules required</li>
<li>A YAML template for your tested provider. See the <a href="networking/../../../templates/cluster-template-antrea.yaml">Antrea template</a> as an example.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-calico"><a class="header" href="#using-calico">Using Calico</a></h1>
<p>This section lists the security rules that must be implemented in the network security groups (NSGs) in order to use <a href="https://www.tigera.io/project-calico/">Calico</a> as a CNI provider.</p>
<h2 id="control-plane-endpoint-nsg"><a class="header" href="#control-plane-endpoint-nsg">Control plane endpoint NSG</a></h2>
<p>The control plane endpoint NSG will be attached to the OCI load balancer. The egress and ingress rules are listed below.</p>
<h3 id="control-plane-endpoint-nsg-egress-rules"><a class="header" href="#control-plane-endpoint-nsg-egress-rules">Control plane endpoint NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>6443</td><td>TCP</td><td>Allow HTTPS traffic to Control plane for Kubernetes API server access</td></tr>
</tbody></table>
</div>
<h3 id="control-plane-endpoint-nsg-ingress-rules"><a class="header" href="#control-plane-endpoint-nsg-ingress-rules">Control plane endpoint NSG ingress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>0.0.0.0/0</td><td>6443</td><td>TCP</td><td>Allow public access to endpoint OCI load balancer</td></tr>
</tbody></table>
</div>
<h2 id="control-plane-nsg"><a class="header" href="#control-plane-nsg">Control plane NSG</a></h2>
<p>The OCI compute instances running the Kubernetes control plane components will be attached to this NSG.</p>
<h3 id="control-plane-nsg-egress-rules"><a class="header" href="#control-plane-nsg-egress-rules">Control plane NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>0.0.0.0/0</td><td>All</td><td>ALL</td><td>Control plane access to Internet to pull images</td></tr>
</tbody></table>
</div>
<h4 id="ingress-rules"><a class="header" href="#ingress-rules">Ingress Rules</a></h4>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>10.0.0.8/29</td><td>6443</td><td>TCP</td><td>Kubernetes API endpoint to Kubernetes control plane communication</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>6443</td><td>TCP</td><td>Control plane to control plane (API server port) communication</td></tr>
<tr><td>CIDR block</td><td>10.0.64.0/20</td><td>6443</td><td>TCP</td><td>Worker Node to Kubernetes control plane (API Server) communication</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>10250</td><td>TCP</td><td>Control Plane to Control Plane Kubelet Communication</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>2379</td><td>TCP</td><td>etcd client communication</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>2380</td><td>TCP</td><td>etcd peer communication</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>179</td><td>TCP</td><td>Calico networking (BGP)</td></tr>
<tr><td>CIDR block</td><td>10.0.64.0/20</td><td>179</td><td>TCP</td><td>Calico networking (BGP)</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td></td><td>IP-in-IP</td><td>Calico networking with IP-in-IP enabled</td></tr>
<tr><td>CIDR block</td><td>10.0.64.0/20</td><td></td><td>IP-in-IP</td><td>Calico networking with IP-in-IP enabled</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/16</td><td></td><td>ICMP Type 3, Code 4</td><td>MTU Path discovery</td></tr>
<tr><td>CIDR block</td><td>0.0.0.0/0</td><td>22</td><td>TCP</td><td>Inbound SSH traffic to control plane nodes</td></tr>
</tbody></table>
</div>
<h2 id="worker-nsg"><a class="header" href="#worker-nsg">Worker NSG</a></h2>
<p>The OCI compute instances which running as Kubernetes worker nodes will be attached to this NSG.</p>
<h3 id="worker-nsg-egress-rules"><a class="header" href="#worker-nsg-egress-rules">Worker NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>0.0.0.0/0</td><td>All</td><td>All</td><td>Worker node access to Internet to pull images</td></tr>
</tbody></table>
</div>
<h3 id="worker-nsg-ingress-rules"><a class="header" href="#worker-nsg-ingress-rules">Worker NSG ingress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>10.0.0.32/27</td><td>32000-32767</td><td>TCP</td><td>Allow incoming traffic from service load balancers (NodePort Communication)</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>10250</td><td>TCP</td><td>Control plane to worker node (Kubelet Communication)</td></tr>
<tr><td>CIDR block</td><td>10.0.64.0/20</td><td>10250</td><td>TCP</td><td>Worker nodes to worker node (Kubelet Communication)</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>179</td><td>TCP</td><td>Calico networking (BGP)</td></tr>
<tr><td>CIDR block</td><td>10.0.64.0/20</td><td>179</td><td>TCP</td><td>Calico networking (BGP)</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td></td><td>IP-in-IP</td><td>Calico networking with IP-in-IP enabled</td></tr>
<tr><td>CIDR block</td><td>10.0.64.0/20</td><td></td><td>IP-in-IP</td><td>Calico networking with IP-in-IP enabled</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/16</td><td></td><td>ICMP Type 3, Code 4</td><td>MTU Path discovery</td></tr>
<tr><td>CIDR block</td><td>0.0.0.0/0</td><td>22</td><td>TCP</td><td>Inbound SSH traffic to worker nodes</td></tr>
</tbody></table>
</div>
<h2 id="service-load-balancers-nsg"><a class="header" href="#service-load-balancers-nsg">Service Load Balancers NSG</a></h2>
<p>OCI load balancers created as part of Kubernetes services of type LoadBalancer will be attached to this NSG.</p>
<h3 id="service-load-balancers-nsg-egress-rules"><a class="header" href="#service-load-balancers-nsg-egress-rules">Service Load Balancers NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>10.0.64.0/20</td><td>32000-32767</td><td>TCP</td><td>Allow access to NodePort services from Service Load balancers</td></tr>
</tbody></table>
</div>
<h3 id="service-load-balancers-nsg-ingress-rules"><a class="header" href="#service-load-balancers-nsg-ingress-rules">Service Load Balancers NSG ingress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR block</td><td>0.0.0.0/0</td><td>80, 443</td><td>TCP</td><td>Allow incoming traffic to services</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="using-antrea"><a class="header" href="#using-antrea">Using Antrea</a></h1>
<p>This section lists the security rules that must be implemented in the Network Security Groups (NSGs) in order to use <a href="https://antrea.io/docs/">Antrea</a> as a CNI provider.</p>
<h2 id="control-plane-endpoint-nsg-1"><a class="header" href="#control-plane-endpoint-nsg-1">Control plane endpoint NSG</a></h2>
<p>The Control plane Endpoint NSG will be attached to the OCI Load Balancer. The egress and ingress rules are listed below.</p>
<h3 id="control-plane-endpoint-nsg-egress-rules-1"><a class="header" href="#control-plane-endpoint-nsg-egress-rules-1">Control plane endpoint NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>6443</td><td>TCP</td><td>Allow HTTPS Traffic to Control plane for Kubernetes API Server access</td></tr>
</tbody></table>
</div>
<h3 id="control-plane-endpoint-nsg-ingress-rules-1"><a class="header" href="#control-plane-endpoint-nsg-ingress-rules-1">Control plane endpoint NSG ingress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>0.0.0.0/0</td><td>6443</td><td>TCP</td><td>Allow public access to endpoint OCI Load Balancer</td></tr>
</tbody></table>
</div>
<h2 id="control-plane-nsg-1"><a class="header" href="#control-plane-nsg-1">Control plane NSG</a></h2>
<p>The OCI Compute instances running the Kubernetes Control plane components will be attached to this NSG.</p>
<h3 id="control-plane-nsg-egress-rules-1"><a class="header" href="#control-plane-nsg-egress-rules-1">Control plane NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>0.0.0.0/0</td><td>All</td><td>ALL</td><td>Control plane access to Internet to pull images</td></tr>
</tbody></table>
</div>
<h3 id="control-plane-nsg-ingress-rules"><a class="header" href="#control-plane-nsg-ingress-rules">Control plane NSG ingress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>10.0.0.8/29</td><td>6443</td><td>TCP</td><td>Kubernetes API endpoint to Kubernetes Control plane communication</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>6443</td><td>TCP</td><td>Control plane to Control plane (API Server port) communication</td></tr>
<tr><td>CIDR Block</td><td>10.0.64.0/20</td><td>6443</td><td>TCP</td><td>Worker Node to Kubernetes Control plane (API Server port)communication</td></tr>
<tr><td>CIDR block</td><td>10.0.0.0/29</td><td>10250</td><td>TCP</td><td>Control Plane to Control Plane Kubelet Communication</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>2379</td><td>TCP</td><td>etcd client communication</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>2380</td><td>TCP</td><td>etcd peer communication</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>10349</td><td>TCP</td><td>Antrea Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.64.0/20</td><td>10349</td><td>TCP</td><td>Antrea Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>6081</td><td>UDP</td><td>Geneve Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.64.0/20</td><td>6081</td><td>UDP</td><td>Geneve Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/16</td><td></td><td>ICMP Type 3, Code 4</td><td>Path discovery</td></tr>
<tr><td>CIDR Block</td><td>0.0.0.0/0</td><td>22</td><td>TCP</td><td>Inbound SSH traffic to Control plane nodes</td></tr>
</tbody></table>
</div>
<h2 id="worker-nsg-1"><a class="header" href="#worker-nsg-1">Worker NSG</a></h2>
<p>The OCI Compute instances which running as Kubernetes worker nodes will be attached to this NSG.</p>
<h3 id="worker-nsg-egress-rules-1"><a class="header" href="#worker-nsg-egress-rules-1">Worker NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>0.0.0.0/0</td><td>All</td><td>All</td><td>Worker Nodes access to Internet to pull images</td></tr>
</tbody></table>
</div>
<h3 id="worker-nsg-ingress-rules-1"><a class="header" href="#worker-nsg-ingress-rules-1">Worker NSG ingress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>10.0.0.32/27</td><td>32000-32767</td><td>TCP</td><td>Allow incoming traffic from service load balancers (NodePort Communication)</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>10250</td><td>TCP</td><td>Control plane to worker node (Kubelet Communication)</td></tr>
<tr><td>CIDR Block</td><td>10.0.64.0/20</td><td>10250</td><td>TCP</td><td>Worker nodes to worker node (Kubelet Communication)</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>10349</td><td>TCP</td><td>Antrea Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.64.0/20</td><td>10349</td><td>TCP</td><td>Antrea Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/29</td><td>6081</td><td>UDP</td><td>Geneve Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.64.0/20</td><td>6081</td><td>UDP</td><td>Geneve Service</td></tr>
<tr><td>CIDR Block</td><td>10.0.0.0/16</td><td></td><td>ICMP Type 3, Code 4</td><td>Path discovery</td></tr>
<tr><td>CIDR Block</td><td>0.0.0.0/0</td><td>22</td><td>TCP</td><td>Inbound SSH traffic to worker nodes</td></tr>
</tbody></table>
</div>
<h2 id="service-load-balancers-nsg-1"><a class="header" href="#service-load-balancers-nsg-1">Service load balancers NSG</a></h2>
<p>OCI load balancers created as part of Kubernetes Services of type LoadBalancer will be attached to this NSG.</p>
<h3 id="service-load-balancers-nsg-egress-rules-1"><a class="header" href="#service-load-balancers-nsg-egress-rules-1">Service load balancers NSG egress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Destination Type</th><th>Destination</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>10.0.64.0/20</td><td>32000-32767</td><td>TCP</td><td>Allow access to NodePort services from Service Load balancers</td></tr>
</tbody></table>
</div>
<h3 id="service-load-balancers-nsg-ingress-rules-1"><a class="header" href="#service-load-balancers-nsg-ingress-rules-1">Service load balancers NSG ingress rules</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Source Type</th><th>Source</th><th>Destination Port</th><th>Protocol</th><th>Description</th></tr></thead><tbody>
<tr><td>CIDR Block</td><td>0.0.0.0/0</td><td>80, 443</td><td>TCP</td><td>Allow incoming traffic to services</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="ipv6-global-unicast-address-gua-allocated-by-oracle-guide"><a class="header" href="#ipv6-global-unicast-address-gua-allocated-by-oracle-guide">IPv6 Global Unicast Address (GUA) Allocated by Oracle Guide</a></h1>
<p>This section contains information about the IPv6 aspects of Cluster API Provider OCI.</p>
<p>In order to have a VM created with IPv6 Ip assigned you should have the following defined:</p>
<ol>
<li>For OCICluster:
<ul>
<li>
<pre><code class="language-yaml">  networkSpec:
      vcn:
          isIpv6Enabled: true
          subnets:
              - ipv6CidrBlockHextet: &quot;01&quot;
</code></pre>
</li>
</ul>
</li>
</ol>
<ul>
<li>example:</li>
</ul>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: &quot;${CLUSTER_NAME}&quot;
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      isIpv6Enabled: true
      subnets:
        - ipv6CidrBlockHextet: &quot;01&quot;
          name: control-plane-endpoint
          role: control-plane-endpoint
          type: public
        - ipv6CidrBlockHextet: &quot;02&quot;
          name: control-plane
          role: control-plane
          type: private
        - ipv6CidrBlockHextet: &quot;03&quot;
          name: service-lb
          role: service-lb
          type: public
        - ipv6CidrBlockHextet: &quot;04&quot;
          name: worker
          role: worker
          type: private
</code></pre>
<ol start="2">
<li>For OCIMachineTemplate:
<ul>
<li>
<pre><code class="language-yaml">  networkDetails:
      assignIpv6Ip: true
</code></pre>
</li>
</ul>
</li>
</ol>
<ul>
<li>example: </li>
</ul>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIMachineTemplate
metadata:
  name: &quot;${CLUSTER_NAME}-md-0&quot;
spec:
  template:
    spec:
      imageId: &quot;${OCI_IMAGE_ID}&quot;
      compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
      shape: &quot;${OCI_NODE_MACHINE_TYPE=VM.Standard.E5.Flex}&quot;
      shapeConfig:
        ocpus: &quot;${OCI_NODE_MACHINE_TYPE_OCPUS=1}&quot;
      metadata:
        ssh_authorized_keys: &quot;${OCI_SSH_KEY}&quot;
      isPvEncryptionInTransitEnabled: ${OCI_NODE_PV_TRANSIT_ENCRYPTION=true}
      networkDetails:
        assignIpv6Ip: true
</code></pre>
<ul>
<li><a href="networking/../../../templates/cluster-template-with-ipv6.yaml">IPv6 Example Template</a></li>
</ul>
<p><strong>Note:</strong></p>
<ul>
<li><strong>VCNs, subnets and VMs can be asigned with IPv6 just at creation step. After creation they can be updated with IPv6 just from UI console.</strong></li>
<li><strong>You cannot create a VM with IPv6 in a subnet/vcn without IPv6 enabled capabilities.</strong></li>
<li><strong>Once VCNs, subnets are enabled with IPv6 cannot unassign their IPv6 CIDRs.</strong></li>
<li><strong>CAPOCI don't support BYOIP IPv6 prefix at the moment.</strong></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-networking"><a class="header" href="#custom-networking">Custom networking</a></h1>
<p>The <a href="networking/infrastructure.html">default networking</a> can be modified to achieve the following:</p>
<ul>
<li>your own CIDR range for VCN. This is useful if you want to perform peering with another VCN or another cloud provider and you need to avoid IP Overlapping</li>
<li>your own custom security rules using <a href="networking/../reference/glossary.html#nsg">NSGs</a>. This is useful if you want to use your own CNI provider and it has a different security posture than the default</li>
<li>your own custom security rules using network security lists</li>
<li>change the masks and name of your different subnets. This is useful to either expand or constrain the size of your subnets as well as to use your own preferred naming convention</li>
</ul>
<p>The <code>OCICluster</code> spec in the cluster templates can be modified to customize the network spec.</p>
<h2 id="example-spec-for-custom-cidr-range"><a class="header" href="#example-spec-for-custom-cidr-range">Example spec for custom CIDR range</a></h2>
<p>The spec below shows how to change the CIDR range of the VCN from the default <code>10.0.0.0/16</code> to <code>172.16.0.0/16</code>.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      name: ${CLUSTER_NAME}
      cidr: &quot;172.16.0.0/16&quot;
      subnets:
        - name: ep-subnet
          role: control-plane-endpoint
          type: public
          cidr: &quot;172.16.0.0/28&quot;
        - name: cp-mc-subnet
          role: control-plane
          type: private
          cidr: &quot;172.16.5.0/28&quot;
        - name: worker-subnet
          role: worker
          type: private
          cidr: &quot;172.16.10.0/24&quot;
        - name: svc-lb-subnet
          role: service-lb
          type: public
          cidr: &quot;172.16.20.0/24&quot;
</code></pre>
<h2 id="example-spec-to-modify-default-nsg-security-rules"><a class="header" href="#example-spec-to-modify-default-nsg-security-rules">Example spec to modify default NSG security rules</a></h2>
<p>The spec below shows how to change the default NSG rules.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      name: ${CLUSTER_NAME}
      cidr: &quot;172.16.0.0/16&quot;
      networkSecurityGroup:
        list:
          - name: ep-nsg
            role: control-plane-endpoint
            egressRules:
              - egressRule:
                  isStateless: false
                  destination: &quot;172.16.5.0/28&quot;
                  protocol: &quot;6&quot;
                  destinationType: &quot;CIDR_BLOCK&quot;
                  description: &quot;All traffic to control plane nodes&quot;
                  tcpOptions:
                    destinationPortRange:
                      max: 6443
                      min: 6443
            ingressRules:
              - ingressRule:
                  isStateless: false
                  source: &quot;0.0.0.0/0&quot;
                  protocol: &quot;6&quot;
                  sourceType: &quot;CIDR_BLOCK&quot;
                  description: &quot;External access to Kubernetes API endpoint&quot;
                  tcpOptions:
                    destinationPortRange:
                      max: 6443
                      min: 6443
              - ingressRule:
                  isStateless: false
                  source: &quot;172.16.5.0/28&quot;
                  protocol: &quot;6&quot;
                  sourceType: &quot;CIDR_BLOCK&quot;
                  description: &quot;Control plane worker nodes to API Server endpoint&quot;
              - ingressRule:
                  isStateless: false
                  source: &quot;0.0.0.0/0&quot;
                  protocol: &quot;6&quot;
                  sourceType: &quot;CIDR_BLOCK&quot;
                  description: &quot;SSH access&quot;
                  tcpOptions:
                    destinationPortRange:
                      max: 22
                      min: 22
          - name: cp-mc-nsg
            role: control-plane
            egressRules:
              - egressRule:
                  isStateless: false
                  destination: &quot;0.0.0.0/0&quot;
                  protocol: &quot;6&quot;
                  destinationType: &quot;CIDR_BLOCK&quot;
                  description: &quot;control plane machine access to internet&quot;
            ingressRules:
              - ingressRule:
                  isStateless: false
                  source: &quot;172.16.0.0/16&quot;
                  protocol: &quot;all&quot;
                  sourceType: &quot;CIDR_BLOCK&quot;
                  description: &quot;Allow inter vcn communication&quot;
              - ingressRule:
                  isStateless: false
                  source: &quot;0.0.0.0/0&quot;
                  protocol: &quot;6&quot;
                  sourceType: &quot;CIDR_BLOCK&quot;
                  description: &quot;SSH access&quot;
                  tcpOptions:
                    destinationPortRange:
                      max: 22
                      min: 22
          - name: worker-nsg
            role: worker
            egressRules:
              - egressRule:
                  isStateless: false
                  destination: &quot;0.0.0.0/0&quot;
                  protocol: &quot;6&quot;
                  destinationType: &quot;CIDR_BLOCK&quot;
                  description: &quot;Worker Nodes access to Internet&quot;
            ingressRules:
              - ingressRule:
                  isStateless: false
                  source: &quot;172.16.0.0/16&quot;
                  protocol: &quot;all&quot;
                  sourceType: &quot;CIDR_BLOCK&quot;
                  description: &quot;Allow inter vcn communication&quot;
          - name: service-lb-nsg
            role: service-lb
            ingressRules:
              - ingressRule:
                  isStateless: false
                  source: &quot;172.16.0.0/16&quot;
                  protocol: &quot;all&quot;
                  sourceType: &quot;CIDR_BLOCK&quot;
                  description: &quot;Allow ingress from vcn subnets&quot;
      subnets:
        - name: ep-subnet
          role: control-plane-endpoint
          type: public
          cidr: &quot;172.16.0.0/28&quot;
        - name: cp-mc-subnet
          role: control-plane
          type: private
          cidr: &quot;172.16.5.0/28&quot;
        - name: worker-subnet
          role: worker
          type: private
          cidr: &quot;172.16.10.0/24&quot;
        - name: svc-lb-subnet
          role: service-lb
          type: public
          cidr: &quot;172.16.20.0/24&quot;
</code></pre>
<h2 id="example-spec-to-use-security-lists-instead-of-network-security-groups"><a class="header" href="#example-spec-to-use-security-lists-instead-of-network-security-groups">Example spec to use Security Lists instead of Network Security Groups</a></h2>
<p>The spec below shows how to implement the security posture using security lists instead of NSGs.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      name: ${CLUSTER_NAME}
      subnets:
        - name: ep-subnet
          role: control-plane-endpoint
          type: public
          securityList:
            name: ep-seclist
            egressRules:
              - destination: &quot;10.0.0.0/29&quot;
                protocol: &quot;6&quot;
                destinationType: &quot;CIDR_BLOCK&quot;
                description: &quot;All traffic to control plane nodes&quot;
                tcpOptions:
                  destinationPortRange:
                    max: 6443
                    min: 6443
            ingressRules:
              - source: &quot;0.0.0.0/0&quot;
                protocol: &quot;6&quot;
                sourceType: &quot;CIDR_BLOCK&quot;
                description: &quot;External access to Kubernetes API endpoint&quot;
                tcpOptions:
                  destinationPortRange:
                    max: 6443
                    min: 6443
              - source: &quot;10.0.0.0/29&quot;
                protocol: &quot;6&quot;
                sourceType: &quot;CIDR_BLOCK&quot;
                description: &quot;Control plane worker nodes to API Server endpoint&quot;
              - source: &quot;0.0.0.0/0&quot;
                protocol: &quot;6&quot;
                sourceType: &quot;CIDR_BLOCK&quot;
                description: &quot;SSH access&quot;
                tcpOptions:
                  destinationPortRange:
                    max: 22
                    min: 22
        - name: cp-mc-subnet
          role: control-plane
          type: private
          securityList:
            name: cp-mc-seclist
            egressRules:
              - destination: &quot;0.0.0.0/0&quot;
                protocol: &quot;6&quot;
                destinationType: &quot;CIDR_BLOCK&quot;
                description: &quot;control plane machine access to internet&quot;
            ingressRules:
              - source: &quot;10.0.0.0/16&quot;
                protocol: &quot;all&quot;
                sourceType: &quot;CIDR_BLOCK&quot;
                description: &quot;Allow inter vcn communication&quot;
              - source: &quot;0.0.0.0/0&quot;
                protocol: &quot;6&quot;
                sourceType: &quot;CIDR_BLOCK&quot;
                description: &quot;SSH access&quot;
                tcpOptions:
                  destinationPortRange:
                    max: 22
                    min: 22
        - name: worker-subnet
          role: worker
          type: private
          securityList:
            name: node-seclist
            egressRules:
              - destination: &quot;0.0.0.0/0&quot;
                protocol: &quot;6&quot;
                destinationType: &quot;CIDR_BLOCK&quot;
                description: &quot;Worker Nodes access to Internet&quot;
            ingressRules:
              - source: &quot;10.0.0.0/16&quot;
                protocol: &quot;all&quot;
                sourceType: &quot;CIDR_BLOCK&quot;
                description: &quot;Allow inter vcn communication&quot;
        - name: svc-lb-subnet
          role: service-lb
          type: public
          securityList:
            name: service-lb-seclist
            ingressRules:
              - source: &quot;10.0.0.0/16&quot;
                protocol: &quot;all&quot;
                sourceType: &quot;CIDR_BLOCK&quot;
                description: &quot;Allow ingress from vcn subnets&quot;
</code></pre>
<p>Related documentation: <a href="https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/securityrules.htm#comparison">comparison of Security Lists and Network Security Groups</a></p>
<h2 id="example-spec-for-externally-managed-vcn-infrastructure-1"><a class="header" href="#example-spec-for-externally-managed-vcn-infrastructure-1">Example spec for externally managed VCN infrastructure</a></h2>
<div id="admonition-info" class="admonition info">
<div class="admonition-title">
<p>Info</p>
<p><a class="admonition-anchor-link" href="networking/custom-networking.html#admonition-info"></a></p>
</div>
<div>
<p>See <a href="networking/../gs/externally-managed-cluster-infrastructure.html#example-spec-for-externally-managed-vcn-infrastructure">externally managed infrastructure</a> for how to create a cluster
using existing VCN infrastructure.</p>
</div>
</div>
<h2 id="example-spec-to-use-oci-load-balancer-as-api-server-load-balancer"><a class="header" href="#example-spec-to-use-oci-load-balancer-as-api-server-load-balancer">Example spec to use OCI Load Balancer as API Server load balancer</a></h2>
<p>By default, CAPOCI uses <a href="https://docs.oracle.com/en-us/iaas/Content/NetworkLoadBalancer/introducton.htm#Overview">OCI Network Load Balancer</a> as API Server load balancer. The load balancer front-ends
control plane hosts to provide high availability access to Kubernetes API. The following spec can be used to 
use <a href="https://docs.oracle.com/en-us/iaas/Content/Balance/Concepts/balanceoverview.htm#Overview_of_Load_Balancing">OCI Load Balancer</a> as the API Server load balancer. The change from the default spec is to set 
<code>loadBalancerType</code> field to &quot;lb&quot; in the <code>OCICluster</code> resource.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    apiServerLoadBalancer:
      loadBalancerType: &quot;lb&quot;
</code></pre>
<h2 id="example-spec-to-use-custom-role"><a class="header" href="#example-spec-to-use-custom-role">Example spec to use custom role</a></h2>
<p>CAPOCI can be used to create Subnet/NSG in the VCN for custom workloads such as private load balancers,
dedicated subnet for DB connection etc. The roles for such custom subnest must be defined as <code>custom</code>.
The following spec shows an example for this scenario.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      name: ${CLUSTER_NAME}
      subnets:
        - name: db
          role: custom
          type: public
          cidr: &quot;172.16.5.0/28&quot;
      networkSecurityGroup:
        list:
          - name: db
            role: custom
            egressRules:
              - egressRule:
                  isStateless: false
                  destination: &quot;172.16.5.0/28&quot;
                  protocol: &quot;6&quot;
                  destinationType: &quot;CIDR_BLOCK&quot;
                  description: &quot;All traffic to control plane nodes&quot;
                  tcpOptions:
                    destinationPortRange:
                      max: 6443
                      min: 6443
</code></pre>
<h2 id="example-spec-to-use-network-security-group-as-destination-in-security-rule"><a class="header" href="#example-spec-to-use-network-security-group-as-destination-in-security-rule">Example spec to use Network Security Group as destination in security rule</a></h2>
<p>The spec below shows how to specify a Network Security Group as a destination in security rule. The Network Security
Group name is mentioned in the <code>destination</code> field in the below example. All the required Network Security Groups 
must be defined in the template, CAPOCI will not lookup Network Security Group from the VCN.</p>
<pre><code class="language-yaml">---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      name: ${CLUSTER_NAME}
      cidr: &quot;172.16.0.0/16&quot;
      networkSecurityGroup:
        list:
          - name: ep-nsg
            role: control-plane-endpoint
            egressRules:
              - egressRule:
                  isStateless: false
                  destination: &quot;cp-mc-nsg&quot;
                  protocol: &quot;6&quot;
                  destinationType: &quot;NETWORK_SECURITY_GROUP&quot;
                  description: &quot;All traffic to control plane nodes&quot;
                  tcpOptions:
                    destinationPortRange:
                      max: 6443
                      min: 6443
          - name: cp-mc-nsg
            role: control-plane
            egressRules:
              - egressRule:
                  isStateless: false
                  destination: &quot;0.0.0.0/0&quot;
                  protocol: &quot;6&quot;
                  destinationType: &quot;CIDR_BLOCK&quot;
                  description: &quot;control plane machine access to internet&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-private-clusters"><a class="header" href="#using-private-clusters">Using private clusters</a></h1>
<blockquote>
<p>Note: This section has to be used only if the CAPOCI manages the workload cluster VCN. If externally managed VCN is
used, this section is not applicable.</p>
</blockquote>
<h2 id="example-spec-for-private-cluster"><a class="header" href="#example-spec-for-private-cluster">Example Spec for private cluster</a></h2>
<p>CAPOCI supports private clusters where the Kubernetes API Server Endpoint is a private IP Address
and is accessible only within the VCN or peered VCNs. In order to use private clusters, the control plane
endpoint subnet has to be marked as private. An example spec is given below.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCICluster
metadata:
  labels:
    cluster.x-k8s.io/cluster-name: &quot;${CLUSTER_NAME}&quot;
  name: &quot;${CLUSTER_NAME}&quot;
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      subnets:
        - cidr: 10.1.0.8/29
          name: control-plane-endpoint
          role: control-plane-endpoint
          type: private
        - cidr: 10.1.0.0/29
          name: control-plane
          role: control-plane
          type: private
        - cidr: 10.1.0.32/27
          name: service-lb
          role: service-lb
          type: public
        - cidr: 10.1.64.0/20
          name: worker
          role: worker
          type: private
</code></pre>
<h2 id="example-spec-for-vcn-peering-using-dynamic-routing-gateway-local"><a class="header" href="#example-spec-for-vcn-peering-using-dynamic-routing-gateway-local">Example spec for VCN Peering using Dynamic Routing Gateway (Local)</a></h2>
<p>While using private clusters, the management cluster needs to talk to the workload cluster. If the
management cluster and workload cluster are in separate VCN, the VCN peering can be used to connect the management
and workload cluster VCNS. CAPOCI supports peering of the workload cluster VCN with another VCN in the same region using
<a href="https://docs.oracle.com/en-us/iaas/Content/Network/Tasks/managingDRGs.htm">Dynamic Routing Gateway</a>.</p>
<p>In case of local VCN peering, a DRG OCID has to be provided and CAPOCI will attach the workload cluster VCN to the
provided DRG. The recommendation is to attach the management cluster VCN also to the same DRG so that the VCNs are
peered to each other. For more details see <a href="https://docs.oracle.com/en-us/iaas/Content/Network/Tasks/localVCNpeering.htm">Local VCN Peering using Local Peering Gateways</a>.</p>
<p>An example template for this <code>cluster-template-local-vcn-peering.yaml</code> can be found in the Assets section under the
<a href="https://github.com/oracle/cluster-api-provider-oci/releases/latest">CAPOCI release page</a>. </p>
<p>In order to use the template, the following Cluster API parameters have to be set in addition to the common parameters 
explained in the <a href="networking/../gs/create-workload-cluster.html#workload-cluster-parameters">Workload Cluster Parameters table</a>.</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>DRG_ID</code></td><td></td><td>OCID of the DRG to which the worklaod cluster will be attached.</td></tr>
</tbody></table>
</div>
<h2 id="example-spec-for-vcn-peering-using-dynamic-routing-gateway-remote"><a class="header" href="#example-spec-for-vcn-peering-using-dynamic-routing-gateway-remote">Example spec for VCN Peering using Dynamic Routing Gateway (Remote)</a></h2>
<p>If the management cluster and workload cluster are in different OCI regions, then DRG can still be used. In this case,
in addition to VCN attachment, <a href="https://docs.oracle.com/en-us/iaas/Content/Network/Tasks/scenario_e.htm">Remote Peering Connection (RPC) </a> has to be used.</p>
<p>In case of remote VCN peering, a DRG will be created by CAPOCI, and the workload cluster VCN will be attached to the
DRG. In addition, a remote DRG has to be provided. CAPOCI will create RPC in the local and remote VCN and
connection will be established between the RPCs.</p>
<p>An example template for this <code>cluster-template-remote-vcn-peering.yaml</code> can be found in the Assets section under the
<a href="https://github.com/oracle/cluster-api-provider-oci/releases/latest">CAPOCI release page</a>. </p>
<p>In order to use the template, the following Cluster API parameters have to be set in addition to the common parameters
explained in the <a href="networking/../gs/create-workload-cluster.html#workload-cluster-parameters">Workload Cluster Parameters table</a>. Typically, the peer DRG refers to the DRG to 
which the management cluster VCN is attached.</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>PEER_DRG_ID</code></td><td></td><td>OCID of the peer DRG to which the local DRG will be peered.</td></tr>
<tr><td><code>PEER_REGION_NAME</code></td><td></td><td>The region to which the peer DRG belongs.</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="managed-clusters-oke"><a class="header" href="#managed-clusters-oke">Managed Clusters (OKE)</a></h1>
<p>Cluster API Provider for OCI (CAPOCI) supports managing OCI Container 
Engine for Kubernetes (OKE) clusters. CAPOCI implements this with three
custom resources:</p>
<ul>
<li><code>OCIManagedControlPlane</code></li>
<li><code>OCIManagedCluster</code></li>
<li><code>OCIManagedMachinePool</code></li>
</ul>
<h2 id="workload-cluster-parameters-1"><a class="header" href="#workload-cluster-parameters-1">Workload Cluster Parameters</a></h2>
<p>The following Oracle Cloud Infrastructure (OCI) configuration parameters are available
when creating a managed workload cluster on OCI using one of our predefined templates:</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Mandatory</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>OCI_COMPARTMENT_ID</code></td><td>Yes</td><td></td><td>The OCID of the compartment in which to create the required compute, storage and network resources.</td></tr>
<tr><td><code>OCI_MANAGED_NODE_IMAGE_ID</code></td><td>No</td><td>&quot;&quot;</td><td>The OCID of the image for the Kubernetes worker nodes. Please read the <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Reference/contengimagesshapes.htm#images">doc</a> for more details.  If no value is specified, a default Oracle Linux OKE platform image is looked up and used</td></tr>
<tr><td><code>OCI_MANAGED_NODE_SHAPE </code></td><td>No</td><td>VM.Standard.E4.Flex</td><td>The <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Reference/contengimagesshapes.htm#shapes">shape</a> of the Kubernetes worker nodes.</td></tr>
<tr><td><code>OCI_MANAGED_NODE_MACHINE_TYPE_OCPUS</code></td><td>No</td><td>1</td><td>The number of OCPUs allocated to the worker node instance.</td></tr>
<tr><td><code>OCI_SSH_KEY</code></td><td>Yes</td><td></td><td>The public SSH key to be added to the Kubernetes nodes. It can be used to login to the node and troubleshoot failures.</td></tr>
</tbody></table>
</div>
<blockquote>
<p>Note: In production use-case, the node pool image id must be provided explicitly and the default lookup mechanism must not be used.</p>
</blockquote>
<p>The following Cluster API parameters are also available:</p>
<div class="table-wrapper"><table><thead><tr><th>Parameter</th><th>Default Value</th><th>Description</th></tr></thead><tbody>
<tr><td><code>CLUSTER_NAME</code></td><td></td><td>The name of the workload cluster to create.</td></tr>
<tr><td><code>KUBERNETES_VERSION</code></td><td></td><td>The Kubernetes version installed on the workload cluster nodes. If this environement variable is not configured, the version must be specified in the <code>.cluster-api/clusterctl.yaml</code> file</td></tr>
<tr><td><code>NAMESPACE</code></td><td></td><td>The namespace for the workload cluster. If not specified, the current namespace is used.</td></tr>
<tr><td><code>NODE_MACHINE_COUNT</code></td><td></td><td>The number of machines in the OKE nodepool.</td></tr>
</tbody></table>
</div>
<h2 id="pre-requisites"><a class="header" href="#pre-requisites">Pre-Requisites</a></h2>
<h3 id="oci-security-policies"><a class="header" href="#oci-security-policies">OCI Security Policies</a></h3>
<p>Please read the <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengpolicyconfig.htm">doc</a> and add the necessary policies required for the user group.
Please add the policies for dynamic groups if instance principal is being used as authentication 
mechanism. Please read the <a href="managed/../gs/install-cluster-api.html">doc</a> to know more about authentication mechanisms.</p>
<h2 id="workload-cluster-templates-1"><a class="header" href="#workload-cluster-templates-1">Workload Cluster Templates</a></h2>
<p>Choose one of the available templates to create your workload clusters from the
<a href="https://github.com/oracle/cluster-api-provider-oci/releases/latest">latest released artifacts</a>. The managed cluster templates is of the
form <code>cluster-template-managed-&lt;flavour&gt;</code>.yaml . The default managed template is
<code>cluster-template-managed.yaml</code>. Please note that the templates provided are to be considered
as references and can be customized further as the <a href="managed/../reference/api-reference.html">CAPOCI API Reference</a>.</p>
<h2 id="supported-kubernetes-versions"><a class="header" href="#supported-kubernetes-versions">Supported Kubernetes versions</a></h2>
<p>The <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengaboutk8sversions.htm#supportedk8sversions">doc</a> lists the Kubernetes versions currently supported by OKE.</p>
<h2 id="create-a-new-oke-cluster"><a class="header" href="#create-a-new-oke-cluster">Create a new OKE cluster.</a></h2>
<p>The following command will create an OKE cluster using the default template. The created node pool uses 
<a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengpodnetworking_topic-OCI_CNI_plugin.htm">VCN native pod networking</a>.</p>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_MANAGED_NODE_IMAGE_ID=&lt;ubuntu-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
KUBERNETES_VERSION=v1.24.1 \
NAMESPACE=default \
clusterctl generate cluster &lt;cluster-name&gt;\
--from cluster-template-managed.yaml | kubectl apply -f -
</code></pre>
<h2 id="create-a-new-private-oke-cluster"><a class="header" href="#create-a-new-private-oke-cluster">Create a new private OKE cluster.</a></h2>
<p>The following command will create an OKE private cluster. In this template, the control plane endpoint subnet is a
private subnet and the API endpoint is accessible only within the subnet. The created node pool uses
<a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Concepts/contengpodnetworking_topic-OCI_CNI_plugin.htm">VCN native pod networking</a>.</p>
<pre><code class="language-bash">OCI_COMPARTMENT_ID=&lt;compartment-id&gt; \
OCI_MANAGED_NODE_IMAGE_ID=&lt;ubuntu-custom-image-id&gt; \
OCI_SSH_KEY=&lt;ssh-key&gt;  \
KUBERNETES_VERSION=v1.24.1 \
NAMESPACE=default \
clusterctl generate cluster &lt;cluster-name&gt;\
--from cluster-template-managedprivate.yaml | kubectl apply -f -
</code></pre>
<h2 id="access-kubeconfig-of-an-oke-cluster"><a class="header" href="#access-kubeconfig-of-an-oke-cluster">Access kubeconfig of an OKE cluster</a></h2>
<h3 id="step-1---identify-cluster-ocid"><a class="header" href="#step-1---identify-cluster-ocid">Step 1 - Identify Cluster OCID</a></h3>
<p>Access the management cluster using kubectl and identify the OKE cluster OCID</p>
<pre><code class="language-bash">kubectl get ocimanagedcontrolplane &lt;workload-cluster-name&gt; -n &lt;workload-cluster-namespace&gt; -o jsonpath='{.spec.id}'
</code></pre>
<h3 id="step-2---access-kubeconfig"><a class="header" href="#step-2---access-kubeconfig">Step 2 - Access kubeconfig</a></h3>
<p>Access kubeconfig</p>
<pre><code class="language-bash">oci ce cluster create-kubeconfig --cluster-id &lt;cluster-ocid&gt; --file &lt;file-name&gt;  --region &lt;region&gt;  --kube-endpoint PUBLIC_ENDPOINT
</code></pre>
<p>Please read the <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengdownloadkubeconfigfile.htm">doc</a> for more details on how to access kubeconfig file of OKE clusters.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="oke-enhanced-clusters-and-virtual-nodes"><a class="header" href="#oke-enhanced-clusters-and-virtual-nodes">OKE Enhanced Clusters and Virtual Nodes</a></h1>
<p>CAPOCI supports OKE <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengcomparingenhancedwithbasicclusters_topic.htm">Enhanced Clusters</a> and <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengworkingwithvirtualnodes.htm">Virtual Nodes</a>. A cluster-template 
<code>cluster-template-managed-virtual-node.yaml</code> with Enhanced Cluster and Virtual Node Pool has been released in 
CAPOCI release artifacts which can be referred using  the flavor <code>managed-virtual-node</code> in <code>clusterctl generate</code> 
command.</p>
<blockquote>
<p>NOTE: Virtual Node Pool require Cluster API release 1.4.3 or later and CAPOCI release 0.10.0 or later.</p>
</blockquote>
<h2 id="create-enhanced-cluster"><a class="header" href="#create-enhanced-cluster">Create Enhanced Cluster</a></h2>
<p>The following <code>OCIManagedControlPlane</code> snippet can be used to create an enhanced OKE cluster.</p>
<pre><code class="language-yaml">kind: OCIManagedControlPlane
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  clusterType: &quot;ENHANCED_CLUSTER&quot;
</code></pre>
<h2 id="create-virtual-node-pool"><a class="header" href="#create-virtual-node-pool">Create Virtual Node Pool</a></h2>
<p>The following <code>OCIVirtualMachinePool</code> snippet can be used to create a Virtual Node Pool. Please read through <a href="managed/../reference/api-reference.html">CAPOCI 
API Docs</a> to see all the supported parameters of <code>OCIVirtualMachinePool</code>.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIVirtualMachinePool
spec:
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="self-managed-nodes"><a class="header" href="#self-managed-nodes">Self managed nodes</a></h1>
<p>CAPOCI supports <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengworkingwithselfmanagednodes.htm">OKE self managed nodes</a>. With this feature, CAPI features such as <a href="https://cluster-api.sigs.k8s.io/tasks/upgrading-clusters.html#upgrading-machines-managed-by-a-machinedeployment">rolling update</a>,
<a href="https://cluster-api.sigs.k8s.io/tasks/automated-machine-management/healthchecking.html">health checks</a> can be used to make management of self managed nodes easier. CAPOCI supports two
flavours fo self managed nodes. It also allows full range of <a href="https://docs.oracle.com/en-us/iaas/api/#/en/iaas/20160918/Instance/LaunchInstance">OCI Compute API</a> to be used while 
creating worker nodes.</p>
<p>Please read the prerequisites related to <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengprereqsforselfmanagednodes.htm">Dynamic Group and policy</a> before creating self managed OKE nodes.</p>
<h1 id="self-managed-nodes-backed-by-capi-machine-deployment"><a class="header" href="#self-managed-nodes-backed-by-capi-machine-deployment">Self managed nodes backed by CAPI Machine Deployment</a></h1>
<p>Use the template <code>cluster-template-managed-self-managed-nodes.yaml</code> as an example for creating and OKE cluster
with a self managed CAPI machine deployment. Self managed nodes are only supported if flannel
is used as CNI provider. The following snippet shows the relevant part of the template.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIMachineTemplate
metadata:
  name: &quot;${CLUSTER_NAME}-md-0&quot;
spec:
  template:
    spec:
      imageId: &quot;${OCI_MANAGED_NODE_IMAGE_ID}&quot;
---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachineDeployment
metadata:
  name: &quot;${CLUSTER_NAME}-md-0&quot;
spec:
  clusterName: &quot;${CLUSTER_NAME}&quot;
  replicas: ${WORKER_MACHINE_COUNT}
  selector:
    matchLabels:
  template:
    spec:
      clusterName: &quot;${CLUSTER_NAME}&quot;
      version: &quot;${KUBERNETES_VERSION}&quot;
      bootstrap:
        dataSecretName: &quot;${CLUSTER_NAME}-self-managed&quot;
      infrastructureRef:
        name: &quot;${CLUSTER_NAME}-md-0&quot;
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: OCIMachineTemplate
</code></pre>
<p>Note that CAPOCI will populate a bootstrap secret with the relevant <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengcloudinitforselfmanagednodes.htm">cloud-init</a> script required 
for the node to join the OKE cluster</p>
<h1 id="self-managed-nodes-backed-by-oci-instance-pool"><a class="header" href="#self-managed-nodes-backed-by-oci-instance-pool">Self managed nodes backed by OCI Instance Pool</a></h1>
<blockquote>
<p>Note: MachinePool is still an experimental feature in CAPI</p>
</blockquote>
<p>The following snippet can be used to create self managed OKE nodes backed by OCI Instance Pool<a href="https://docs.oracle.com/en-us/iaas/Content/Compute/Concepts/instancemanagement.htm#Instance">instance-pool</a>.</p>
<pre><code class="language-yaml">---
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: &quot;${CLUSTER_NAME}-mp-0&quot;
  namespace: default
spec:
  clusterName: &quot;${CLUSTER_NAME}&quot;
  replicas: &quot;${WORKER_MACHINE_COUNT}&quot;
  template:
    spec:
      bootstrap:
        dataSecretName: &quot;${CLUSTER_NAME}-self-managed&quot;
      clusterName: &quot;${CLUSTER_NAME}&quot;
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: OCIMachinePool
        name: &quot;${CLUSTER_NAME}-mp-0&quot;
      version: &quot;${KUBERNETES_VERSION}&quot;
---
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIMachinePool
metadata:
  name: &quot;${CLUSTER_NAME}-mp-0&quot;
  namespace: default
spec:
  instanceConfiguration:
    metadata:
      ssh_authorized_keys: &quot;${OCI_SSH_KEY}&quot;
    instanceSourceViaImageConfig:
      imageId: &quot;${OCI_MANAGED_NODE_IMAGE_ID}&quot;
    shape: &quot;${OCI_NODE_MACHINE_TYPE=VM.Standard.E4.Flex}&quot;
    shapeConfig:
      ocpus: &quot;1&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="increase-boot-volume"><a class="header" href="#increase-boot-volume">Increase boot volume</a></h1>
<p>The default boot volume size of worker nodes is 50 GB. The following steps needs to be followed
to increase the boot volume size.</p>
<h2 id="increase-the-boot-volume-size-in-spec"><a class="header" href="#increase-the-boot-volume-size-in-spec">Increase the boot volume size in spec</a></h2>
<p>The following snippet shows how to increase the boot volume size of the instances.</p>
<pre><code class="language-yaml">kind: OCIManagedMachinePool
spec:
  nodeSourceViaImage:
    bootVolumeSizeInGBs: 100
</code></pre>
<h2 id="extend-the-root-partition"><a class="header" href="#extend-the-root-partition">Extend the root partition</a></h2>
<p>In order to take advantage of the larger size, you need to <a href="https://docs.oracle.com/en-us/iaas/Content/Block/Tasks/extendingbootpartition.htm">extend the partition for the boot volume</a>.
Custom cloud init scripts can be used for the same. The following cloud init script extends the root volume.</p>
<pre><code class="language-bash">#!/bin/bash

# DO NOT MODIFY
curl --fail -H &quot;Authorization: Bearer Oracle&quot; -L0 http://169.254.169.254/opc/v2/instance/metadata/oke_init_script | base64 --decode &gt;/var/run/oke-init.sh

## run oke provisioning script
bash -x /var/run/oke-init.sh

### adjust block volume size
/usr/libexec/oci-growfs -y

touch /var/log/oke.done
</code></pre>
<p>Encode the file contents into a base64 encoded value as follows.</p>
<pre><code class="language-bash">cat cloud-init.sh | base64 -w 0
</code></pre>
<p>Add the value in the following <code>OCIManagedMachinePool</code> spec.</p>
<pre><code class="language-yaml">kind: OCIManagedMachinePool
spec:
  nodeMetadata:
    user_data: &quot;&lt;base64 encoded value from above&gt;&quot;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="networking-customizations"><a class="header" href="#networking-customizations">Networking customizations</a></h1>
<h2 id="use-a-pre-existing-vcn"><a class="header" href="#use-a-pre-existing-vcn">Use a pre-existing VCN</a></h2>
<p>The following <code>OCIManagedCluster</code> snippet can be used to to use a pre-existing VCN.</p>
<pre><code class="language-yaml">kind: OCIManagedCluster
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    skipNetworkManagement: true
    vcn:
      id: &quot;&lt;vcn-id&gt;&quot;
      networkSecurityGroup:
        list:
          - id: &quot;&lt;control-plane-endpoint-nsg-id&gt;&quot;
            role: control-plane-endpoint
            name: control-plane-endpoint
          - id:  &quot;&lt;worker-nsg-id&gt;&quot;
            role: worker
            name: worker
          - id: &quot;&lt;pod-nsg-id&gt;&quot;
            role: pod
            name: pod
      subnets:
        - id: &quot;&lt;control-plane-endpoint-subnet-id&gt;&quot;
          role: control-plane-endpoint
          name: control-plane-endpoint
          type: public
        - id: &quot;&lt;worker-subnet-id&gt;&quot;
          role: worker
          name: worker
        - id: &quot;&lt;pod-subnet-id&gt;&quot;
          role: pod
          name: pod
        - id: &quot;&lt;service-lb-subnet-id&gt;&quot;
          role: service-lb
          name: service-lb
          type: public
</code></pre>
<h2 id="use-a-pre-existing-vcn-subnet-and-gatways-but-the-other-networking-components-self-managed"><a class="header" href="#use-a-pre-existing-vcn-subnet-and-gatways-but-the-other-networking-components-self-managed">Use a pre-existing VCN, Subnet and Gatways, but the other networking components self managed</a></h2>
<p>The following <code>OCIManagedCluster</code> example spec is given below</p>
<pre><code class="language-yaml">kind: OCIManagedCluster
spec:
  compartmentId: &quot;${OCI_COMPARTMENT_ID}&quot;
  networkSpec:
    vcn:
      skip: true
      id: &lt;Insert VCN OCID Here&gt; // REQUIRED
      networkSecurityGroup:
        skip: false
      internetGateway:
        skip: true // REQUIRED
      natGateway:
        skip: true // REQUIRED
      serviceGateway:
        skip: true // REQUIRED
      routeTable:
        skip: true // REQUIRED
      subnets:
        - id: &lt;Insert control Plane Subnet OCID Here&gt; // REQUIRED
          role: control-plane-endpoint
          name: control-plane-endpoint
          type: public
          skip: true
        - id: &lt;Insert control Plane Subnet OCID Here&gt; // REQUIRED
          role: worker
          name: worker
          type: private
          skip: true
        - id: &lt;Insert control Plane Subnet OCID Here&gt; // REQUIRED
          role: control-plane
          name: control-plane
          type: private
          skip: true
        - id: &lt;Insert control Plane Subnet OCID Here&gt; // REQUIRED
          role: service-lb
          name: service-lb
          type: public
          skip: true
</code></pre>
<h2 id="use-flannel-as-cni"><a class="header" href="#use-flannel-as-cni">Use flannel as CNI</a></h2>
<p>Use the template <code>cluster-template-managed-flannel.yaml</code> as an example for using flannel as the CNI. The template
sets the correct parameters in the spec as well as create the proper security roles in the Network Security Group (NSG).</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="features-1"><a class="header" href="#features-1">Features</a></h1>
<p>This page will cover configuration of various Oracle Cloud Infrastructure Container Engine for Kubernetes (OKE) 
features in CAPOCI.</p>
<h2 id="node-pool-cycling"><a class="header" href="#node-pool-cycling">Node Pool Cycling</a></h2>
<p>OKE <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengupgradingk8sworkernode_topic-Performing_an_InPlace_Worker_Node_Upgrade_by_Cycling_an_Existing_Node_Pool.htm#contengupgradingk8sworkernode_topic-Performing_an_InPlace_Worker_Node_Upgrade_by_Cycling_an_Existing_Node_Pool">Node Pool Cycling</a> can be used during Kubernetes version upgrades to cycle
the nodes such that all the nodes of a Node Pool is running on the newer Kubernetes version. The following
<code>OCIManagedMachinePool</code> spec can be used to specify Node Pool cycling option.</p>
<pre><code class="language-yaml">apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: OCIManagedMachinePool
spec:
  nodePoolCyclingDetails:
    isNodeCyclingEnabled: true
</code></pre>
<h2 id="addons"><a class="header" href="#addons">Addons</a></h2>
<p>The following <code>OCIManagedControlPlane</code> spec should  be used to specify <a href="https://docs.oracle.com/en-us/iaas/Content/ContEng/Tasks/contengconfiguringclusteraddons.htm">Addons</a> which has to be
installed in the OKE cluster.</p>
<pre><code class="language-yaml">kind: OCIManagedControlPlane
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
spec:
  clusterType: &quot;ENHANCED_CLUSTER&quot;
  addons:
  - name: CertManager
</code></pre>
<p>More details about the configuration parameters are available in [CAPOCI API Reference docs][api-reference].</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<p>This section contains various resources that define the Cluster API for OCI project.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="api-reference"><a class="header" href="#api-reference">API Reference</a></h1>
<p>Versioned Cluster API Provider for OCI API references is published <a href="https://doc.crds.dev/github.com/oracle/cluster-api-provider-oci">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h1>
<p><a href="reference/glossary.html#a">A</a> | <a href="reference/glossary.html#b">B</a> | <a href="reference/glossary.html#c">C</a> | <a href="reference/glossary.html#d">D</a> | <a href="reference/glossary.html#h">H</a> | <a href="reference/glossary.html#i">I</a> | <a href="reference/glossary.html#k">K</a> | <a href="reference/glossary.html#m">M</a> | <a href="reference/glossary.html#n">N</a> | <a href="reference/glossary.html#o">O</a> | <a href="reference/glossary.html#p">P</a> | <a href="reference/glossary.html#r">R</a> | <a href="reference/glossary.html#s">S</a> | <a href="reference/glossary.html#t">T</a> | <a href="reference/glossary.html#w">W</a></p>
<h1 id="a"><a class="header" href="#a">A</a></h1>
<hr />
<h3 id="ad"><a class="header" href="#ad">AD</a></h3>
<p>Or <strong>Availability Domain</strong> </p>
<p>One or more isolated, fault-tolerant Oracle data centers that host cloud resources such as instances, volumes, and subnets. A region contains one or more availability domains.</p>
<h1 id="c"><a class="header" href="#c">C</a></h1>
<hr />
<h3 id="cni"><a class="header" href="#cni">CNI</a></h3>
<p>Or <strong>Container Network Interface</strong></p>
<p>A <a href="https://cncf.io">Cloud Native Computing Foundation</a> project that  consists of a specification and libraries for 
writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins.</p>
<h1 id="f"><a class="header" href="#f">F</a></h1>
<hr />
<h3 id="fd"><a class="header" href="#fd">FD</a></h3>
<p>Or <strong>Fault Domain</strong> </p>
<p>A logical grouping of hardware and infrastructure within an <a href="reference/glossary.html#ad">availability domain</a>. Fault domains isolate resources during hardware failure or unexpected software changes.</p>
<h1 id="i"><a class="header" href="#i">I</a></h1>
<hr />
<h3 id="internet-gateway"><a class="header" href="#internet-gateway">Internet Gateway</a></h3>
<p>An Internet Gateway is an optional virtual router you can add to your <a href="reference/glossary.html#vcn">VCN</a> to enable direct connectivity to the Internet. It supports connections initiated from within the VCN (egress) and connections initiated from the Internet (ingress).</p>
<h1 id="n"><a class="header" href="#n">N</a></h1>
<hr />
<h3 id="nat-gateway"><a class="header" href="#nat-gateway">NAT Gateway</a></h3>
<p>A NAT Gateway gives cloud resources without public IP addresses access to the Internet without exposing those resources to incoming internet connections. A NAT Gateway can be added to a <a href="reference/glossary.html#vcn">VCN</a> to give instances in a private subnet access to the Internet.</p>
<h3 id="nsg"><a class="header" href="#nsg">NSG</a></h3>
<p>Or <strong>Network Security Group</strong></p>
<p>A <a href="https://docs.oracle.com/en-us/iaas/Content/Network/Concepts/networksecuritygroups.htm">Network security group (NSG)</a> acts as a virtual firewall for your compute instances and other kinds of resources. An NSG consists of a set of ingress and egress security rules that apply only to a set of VNICs of your choice in a single VCN (for example: all compute instances that act as web servers in the web tier of a multi-tier application in your VCN).</p>
<h1 id="r"><a class="header" href="#r">R</a></h1>
<hr />
<h3 id="region"><a class="header" href="#region">Region</a></h3>
<p>Oracle Cloud Infrastructure is hosted in regions and availability domains. A <a href="https://docs.oracle.com/en-us/iaas/Content/General/Concepts/regions.htm">region</a> is a localized geographic area, and an <a href="reference/glossary.html#ad">availability domain</a> is one or more data centers located within a region. A region is composed of one or more availability domains.</p>
<h1 id="s"><a class="header" href="#s">S</a></h1>
<hr />
<h3 id="service-gateway"><a class="header" href="#service-gateway">Service Gateway</a></h3>
<p>A service gateway lets your <a href="reference/glossary.html#vcn">VCN</a> privately access specific Oracle services without exposing the data to the public Internet. No <a href="reference/glossary.html#internet-gateway">Internet Gateway</a> or <a href="reference/glossary.html#nat-gateway">NAT</a> is required to reach those specific services. The resources in the VCN can be in a private subnet and use only private IP addresses. The traffic from the VCN to the Oracle service travels over the Oracle network fabric and never traverses the Internet.</p>
<h1 id="v"><a class="header" href="#v">V</a></h1>
<hr />
<h3 id="vcn"><a class="header" href="#vcn">VCN</a></h3>
<p>Or <strong>Virtual Cloud Network</strong></p>
<p>A <a href="https://docs.oracle.com/en-us/iaas/Content/Network/Tasks/managingVCNs_topic-Overview_of_VCNs_and_Subnets.htm#Overview">VCN</a> is a software-defined network that you set up in the Oracle Cloud Infrastructure data centers in a particular <a href="reference/glossary.html#region">region</a>.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
    </body>
</html>
